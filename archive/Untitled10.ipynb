{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conda env=/gpfs/milgram/project/turk-browne/users/kp578/CONDA/rtcloud\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.sum(chosenMask)=4927\n",
      "1--LENGTH OF TR: 80\n",
      "shape of features (80, 4927) shape of chosenMask (94, 94, 72)\n",
      "2--LENGTH OF TR: 80\n",
      "shape of features (80, 4927) shape of chosenMask (94, 94, 72)\n",
      "3--LENGTH OF TR: 80\n",
      "shape of features (80, 4927) shape of chosenMask (94, 94, 72)\n",
      "4--LENGTH OF TR: 80\n",
      "shape of features (80, 4927) shape of chosenMask (94, 94, 72)\n",
      "5--LENGTH OF TR: 80\n",
      "shape of features (80, 4927) shape of chosenMask (94, 94, 72)\n",
      "6--LENGTH OF TR: 80\n",
      "shape of features (80, 4927) shape of chosenMask (94, 94, 72)\n",
      "(480, 4927)\n",
      "(480, 2)\n",
      "FEAT.shape=(480, 4927)\n",
      "new trained 4 way classifier accuracy=0.3520833333333333\n",
      "best 4way classifier accuracy =  0.3520833333333333\n",
      "bedbench_bedchair 0.5\n",
      "bedbench_bedtable 0.45\n",
      "bedbench_benchchair 0.575\n",
      "bedbench_benchtable 0.575\n",
      "bedchair_bedbench 0.6\n",
      "bedchair_bedtable 0.45\n",
      "bedchair_chairbench 0.575\n",
      "bedchair_chairtable 0.475\n",
      "bedtable_bedbench 0.6\n",
      "bedtable_bedchair 0.5\n",
      "bedtable_tablebench 0.575\n",
      "bedtable_tablechair 0.475\n",
      "benchchair_benchbed 0.6\n",
      "benchchair_benchtable 0.575\n",
      "benchchair_chairbed 0.5\n",
      "benchchair_chairtable 0.475\n",
      "benchtable_benchbed 0.6\n",
      "benchtable_benchchair 0.575\n",
      "benchtable_tablebed 0.45\n",
      "benchtable_tablechair 0.475\n",
      "chairtable_chairbed 0.5\n",
      "chairtable_chairbench 0.575\n",
      "chairtable_tablebed 0.45\n",
      "chairtable_tablebench 0.575\n",
      "6--LENGTH OF TR: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/25 [00:27<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of features (80, 4927) shape of mask (94, 94, 72)\n",
      "(80, 4927)\n",
      "(80, 2)\n",
      "FEAT.shape=(80, 4927)\n",
      "floor\n",
      "D evidence for AD_clf when A is presented=-0.28403150374256286\n",
      "C evidence for AC_clf when A is presented=-0.08419816188336389\n",
      "D evidence for CD_clf when A is presented=-0.1040101817607326\n",
      "C evidence for CD_clf when A is presented=0.1040101817607326\n",
      "ceil\n",
      "A evidence in AC_clf when A is presented=0.08419816188336389\n",
      "A evidence in AD_clf when A is presented=0.28403150374256286\n",
      "evidence_ceil=0.1841148328129634\n",
      "floor=0.1040101817607326, ceil=0.1841148328129634\n",
      "mu=0.144062507286848, sig=0.03401760279099321\n",
      "classifierEvidence(BC_clf,FEAT,Y)=[[ 0.3710985 ]\n",
      " [-0.26933794]\n",
      " [-1.29443766]\n",
      " [-0.48907467]\n",
      " [ 0.51176747]\n",
      " [ 0.94729374]\n",
      " [-1.13527576]\n",
      " [-1.34235417]\n",
      " [ 0.38734985]\n",
      " [-0.76875276]\n",
      " [-0.13564614]\n",
      " [ 0.86744742]\n",
      " [ 0.46313021]\n",
      " [ 0.08375515]\n",
      " [-0.17943085]\n",
      " [-0.4851268 ]\n",
      " [ 1.20167094]\n",
      " [ 1.20290167]\n",
      " [ 0.21921218]\n",
      " [ 0.88492599]\n",
      " [-1.49935934]\n",
      " [ 1.86395713]\n",
      " [ 0.91451205]\n",
      " [-0.02685666]\n",
      " [-0.31547411]\n",
      " [ 0.639639  ]\n",
      " [ 0.35201364]\n",
      " [ 0.96063457]\n",
      " [ 0.90622903]\n",
      " [ 0.32242826]\n",
      " [ 1.27952018]\n",
      " [-0.2789295 ]\n",
      " [-0.17904574]\n",
      " [-0.6781123 ]\n",
      " [ 0.51429437]\n",
      " [ 0.11329606]\n",
      " [ 2.00440398]\n",
      " [ 1.52999003]\n",
      " [ 1.51915754]\n",
      " [-0.1390759 ]\n",
      " [ 0.16472505]\n",
      " [ 0.57886995]\n",
      " [ 1.23553942]\n",
      " [ 2.45865859]\n",
      " [ 1.94891218]\n",
      " [ 2.0365598 ]\n",
      " [ 1.28122842]\n",
      " [ 0.99616463]\n",
      " [ 0.31679142]\n",
      " [ 0.2398161 ]\n",
      " [ 0.99997717]\n",
      " [-2.8458488 ]\n",
      " [-0.57252646]\n",
      " [-0.69775741]\n",
      " [ 0.22376321]\n",
      " [-1.54813353]\n",
      " [ 0.0619341 ]\n",
      " [ 0.19308891]\n",
      " [ 0.24068469]\n",
      " [ 1.33713526]\n",
      " [ 0.50014352]\n",
      " [-0.05473478]\n",
      " [-1.44010989]\n",
      " [-1.54448196]\n",
      " [-1.69605474]\n",
      " [ 0.27077789]\n",
      " [-0.28492688]\n",
      " [-0.53806908]\n",
      " [-1.19425868]\n",
      " [-0.81560605]\n",
      " [ 0.13391393]\n",
      " [ 0.68867442]\n",
      " [-1.70190679]\n",
      " [-1.11411121]\n",
      " [-1.16365519]\n",
      " [-1.16331948]\n",
      " [-2.18953927]\n",
      " [-0.18740283]\n",
      " [-1.09600205]\n",
      " [-0.32476681]]\n",
      "classifierEvidence(BD_clf,FEAT,Y)=[[ 1.31511512]\n",
      " [-1.37616306]\n",
      " [-2.35532339]\n",
      " [-1.12612667]\n",
      " [ 0.57388287]\n",
      " [-0.02635712]\n",
      " [-0.31979239]\n",
      " [-0.66074246]\n",
      " [-0.85127837]\n",
      " [-0.7723339 ]\n",
      " [ 0.21131665]\n",
      " [ 0.14750683]\n",
      " [ 1.51071208]\n",
      " [-0.47149028]\n",
      " [ 1.39929147]\n",
      " [ 0.32973298]\n",
      " [ 1.09284248]\n",
      " [ 0.67534065]\n",
      " [ 0.76091266]\n",
      " [ 0.08880919]\n",
      " [ 0.28224119]\n",
      " [ 1.0003575 ]\n",
      " [ 1.44080406]\n",
      " [ 0.98822845]\n",
      " [-0.61851454]\n",
      " [ 0.75891439]\n",
      " [-1.93908306]\n",
      " [ 0.09591608]\n",
      " [ 0.72894305]\n",
      " [-0.65965547]\n",
      " [ 1.60200233]\n",
      " [-1.04678402]\n",
      " [-0.20677062]\n",
      " [-1.2856162 ]\n",
      " [-0.33898151]\n",
      " [ 0.20777175]\n",
      " [ 0.50167124]\n",
      " [ 0.60211554]\n",
      " [ 1.69927003]\n",
      " [-0.89993047]\n",
      " [-0.89547767]\n",
      " [ 1.97662702]\n",
      " [ 0.69793564]\n",
      " [-0.10291594]\n",
      " [ 0.86618171]\n",
      " [ 0.86824397]\n",
      " [-0.33914859]\n",
      " [ 1.04439815]\n",
      " [ 1.05489528]\n",
      " [ 0.77884622]\n",
      " [ 1.21648385]\n",
      " [-0.117906  ]\n",
      " [ 0.02181512]\n",
      " [ 0.86972864]\n",
      " [ 0.6903558 ]\n",
      " [-0.54759469]\n",
      " [ 0.17769286]\n",
      " [-0.05884973]\n",
      " [-0.61106442]\n",
      " [-0.91455837]\n",
      " [-0.21881043]\n",
      " [ 0.70531524]\n",
      " [-1.17633673]\n",
      " [-1.36272714]\n",
      " [-1.67895772]\n",
      " [ 1.3938086 ]\n",
      " [ 1.12320671]\n",
      " [ 0.3950747 ]\n",
      " [ 1.03775851]\n",
      " [-0.17865246]\n",
      " [ 0.7273081 ]\n",
      " [-1.09850476]\n",
      " [-2.13291936]\n",
      " [ 0.15153184]\n",
      " [-2.01169817]\n",
      " [-1.05867196]\n",
      " [-2.38591838]\n",
      " [-0.90349671]\n",
      " [-0.91923688]\n",
      " [-0.61776619]]\n",
      "BC_B_evidence=[[-0.48907467]\n",
      " [ 0.51176747]\n",
      " [-0.13564614]\n",
      " [ 0.46313021]\n",
      " [ 0.88492599]\n",
      " [-0.02685666]\n",
      " [ 0.96063457]\n",
      " [ 0.32242826]\n",
      " [-0.17904574]\n",
      " [-0.1390759 ]\n",
      " [ 0.57886995]\n",
      " [ 0.99616463]\n",
      " [ 0.99997717]\n",
      " [ 0.22376321]\n",
      " [ 0.24068469]\n",
      " [ 0.50014352]\n",
      " [-0.53806908]\n",
      " [-1.19425868]\n",
      " [-1.16331948]\n",
      " [-2.18953927]]\n",
      "BD_B_evidence=[[-1.12612667]\n",
      " [ 0.57388287]\n",
      " [ 0.21131665]\n",
      " [ 1.51071208]\n",
      " [ 0.08880919]\n",
      " [ 0.98822845]\n",
      " [ 0.09591608]\n",
      " [-0.65965547]\n",
      " [-0.20677062]\n",
      " [-0.89993047]\n",
      " [ 1.97662702]\n",
      " [ 1.04439815]\n",
      " [ 1.21648385]\n",
      " [ 0.6903558 ]\n",
      " [-0.61106442]\n",
      " [-0.21881043]\n",
      " [ 0.3950747 ]\n",
      " [ 1.03775851]\n",
      " [-1.05867196]\n",
      " [-2.38591838]]\n",
      "B_evidence=[[-0.80760067]\n",
      " [ 0.54282517]\n",
      " [ 0.03783526]\n",
      " [ 0.98692115]\n",
      " [ 0.48686759]\n",
      " [ 0.4806859 ]\n",
      " [ 0.52827533]\n",
      " [-0.1686136 ]\n",
      " [-0.19290818]\n",
      " [-0.51950318]\n",
      " [ 1.27774849]\n",
      " [ 1.02028139]\n",
      " [ 1.10823051]\n",
      " [ 0.4570595 ]\n",
      " [-0.18518987]\n",
      " [ 0.14066654]\n",
      " [-0.07149719]\n",
      " [-0.07825009]\n",
      " [-1.11099572]\n",
      " [-2.28772882]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types (dtype('<U506'), dtype('<U506')) -> dtype('<U506')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7bad1f3fcf5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubject\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubjects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7bad1f3fcf5a>\u001b[0m in \u001b[0;36msubLoop\u001b[0;34m(subject)\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m     \u001b[0mfloor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmorphingTarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestRun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"store testing run\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[0mfloor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmorphingTarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestRun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7bad1f3fcf5a>\u001b[0m in \u001b[0;36mmorphingTarget\u001b[0;34m(subject, testRun)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0mB_evidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBC_B_evidence\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBD_B_evidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"B_evidence={B_evidence}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m     \u001b[0mstore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mB_evidence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"mu={mu}, sig={sig}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgaussian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('<U506'), dtype('<U506')) -> dtype('<U506')"
     ]
    }
   ],
   "source": [
    "'''\n",
    "这个code的目的是用neurosketch 的数据来检测现在在realtime data里面发现的issue：也就是ceiling有时候竟然比floor更小\n",
    "这个code的运行逻辑是\n",
    "用neurosketch前五个run训练2 way classifiers，然后用最后一个run来计算ceiling和floor的值，看是否合理\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "purpose:\n",
    "    find the best performed mask from the result of aggregate_greedy.py and save as chosenMask\n",
    "    train all possible pairs of 2way classifiers and save for evidence calculation\n",
    "    load saved classifiers and calculate different forms of evidence\n",
    "steps:\n",
    "    load the result of aggregate_greedy.py\n",
    "    display the result of aggregate_greedy.py\n",
    "    find the best performed ROI for each subject and display the accuracy of each subject, save the best performed ROI as chosenMask\n",
    "    load the functional and behavior data and choseMask and train all possible pairs of 2way classifiers\n",
    "    calculate the evidence floor and ceil for each subject and display different forms of evidences.\n",
    "    \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "load the result of aggregate_greedy.py\n",
    "'''\n",
    "# To visualize the greedy result starting for 31 ROIs, in total 25 subjects.\n",
    "import os\n",
    "os.chdir(\"/gpfs/milgram/project/turk-browne/projects/rtTest/kp_scratch/\")\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle5 as pickle\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import os\n",
    "print(f\"conda env={os.environ['CONDA_DEFAULT_ENV']}\") \n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import itertools\n",
    "import pickle\n",
    "import subprocess\n",
    "from subprocess import call\n",
    "workingDir=\"/gpfs/milgram/project/turk-browne/projects/rtTest/\"\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "roiloc=\"schaefer2018\"\n",
    "dataSource=\"neurosketch\"\n",
    "subjects_correctly_aligned=['1206161','0119173','1206162','1130161','1206163','0120171','0111171','1202161','0125172','0110172','0123173','0120173','0110171','0119172','0124171','0123171','1203161','0118172','0118171','0112171','1207162','0117171','0119174','0112173','0112172']\n",
    "subjects=subjects_correctly_aligned\n",
    "N=25\n",
    "workingPath=\"/gpfs/milgram/project/turk-browne/projects/rtTest/\"\n",
    "GreedyBestAcc=np.zeros((len(subjects),N+1))\n",
    "GreedyBestAcc[GreedyBestAcc==0]=None\n",
    "GreedyBestAcc={}\n",
    "numberOfROIs={}\n",
    "for ii,subject in enumerate(subjects):\n",
    "    # try:\n",
    "    #     GreedyBestAcc[ii,N]=np.load(workingPath+\"./{}/{}/output/uniMaskRanktag2_top{}.npy\".format(roiloc, subject, N))\n",
    "    # except:\n",
    "    #     pass\n",
    "    t=np.load(workingPath+\"./{}/{}/output/uniMaskRanktag2_top{}.npy\".format(roiloc, subject, N))\n",
    "    GreedyBestAcc[subject]=[np.float(t)]\n",
    "    numberOfROIs[subject]=[N]\n",
    "    # for len_topN_1 in range(N-1,0,-1):\n",
    "    for len_topN in range(1,N):\n",
    "        # Wait(f\"./tmp/{subject}_{N}_{roiloc}_{dataSource}_{len_topN_1}.pkl\")\n",
    "        try:\n",
    "            # {当前的被试}_{greedy开始的ROI数目，也就是25}_{mask的种类schaefer2018}_{数据来源neurosketch}_{当前的 megaROI 包含有的数目}\n",
    "            di = load_obj(f\"./tmp__folder/{subject}_{N}_{roiloc}_{dataSource}_{len_topN}\")\n",
    "            GreedyBestAcc[subject].append(np.float(di['bestAcc']))\n",
    "            numberOfROIs[subject].append(len_topN)\n",
    "            # GreedyBestAcc[ii,len_topN] = di['bestAcc']\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "# '''\n",
    "# to load the imtermediate results from greedy code to examine the system\n",
    "# '''\n",
    "# def wait(tmpFile):\n",
    "#     while not os.path.exists(tmpFile+'_result.npy'):\n",
    "#         time.sleep(5)\n",
    "#         print(f\"waiting for {tmpFile}_result.npy\\n\")\n",
    "#     return np.load(tmpFile+'_result.npy')\n",
    "\n",
    "# subject= '0119173' #sys.argv[1]\n",
    "# sub_id = [i for i,x in enumerate(subjects) if x == subject][0]\n",
    "# intermediate_result=np.zeros((N+1,N+1))\n",
    "# # 应该有多少？25个24ROI，2个1ROI，24个\n",
    "# for i in range(N,1,-1):\n",
    "#     for j in range(i):\n",
    "#         tmpFile=f\"./tmp__folder/{subject}_{N}_{roiloc}_{dataSource}_{i}_{j}\"\n",
    "#         sl_result=wait(tmpFile)\n",
    "#         intermediate_result[i,j]=sl_result\n",
    "\n",
    "# # _=plt.imshow(intermediate_result)\n",
    "# #最后一行是25个24ROI，第2行是2个1ROI\n",
    "\n",
    "'''\n",
    "display the result of aggregate_greedy.py\n",
    "'''\n",
    "# GreedyBestAcc=GreedyBestAcc.T\n",
    "# plt.imshow(GreedyBestAcc)\n",
    "# _=plt.figure()\n",
    "# for i in range(GreedyBestAcc.shape[0]):\n",
    "#     plt.scatter([i]*GreedyBestAcc.shape[1],GreedyBestAcc[i,:],c='g',s=2)\n",
    "# plt.plot(np.arange(GreedyBestAcc.shape[0]),np.nanmean(GreedyBestAcc,axis=1))\n",
    "# # plt.ylim([0.19,0.36])\n",
    "# # plt.xlabel(\"number of ROIs\")\n",
    "# # plt.ylabel(\"accuracy\")\n",
    "# _=plt.figure()\n",
    "# for j in range(GreedyBestAcc.shape[1]):\n",
    "#     plt.plot(GreedyBestAcc[:,j])\n",
    "\n",
    "\n",
    "# GreedyBestAcc=GreedyBestAcc.T\n",
    "# _=plt.figure()\n",
    "# plt.imshow(GreedyBestAcc)\n",
    "\n",
    "'''\n",
    "find the best performed ROI for each subject and display the accuracy of each subject, save the best performed ROI as chosenMask\n",
    "'''\n",
    "#find best ID for each subject\n",
    "bestID={}\n",
    "for ii,subject in enumerate(subjects):\n",
    "    t=GreedyBestAcc[subject]\n",
    "    bestID[subject] = numberOfROIs[subject][np.where(t==np.nanmax(t))[0][0]] #bestID 指的是每一个subject对应的最好的megaROI包含的ROI的数目\n",
    "chosenMask={}\n",
    "for subject in bestID:\n",
    "    # best ID  \n",
    "    # {当前的被试}_{greedy开始的ROI数目，也就是25}_{mask的种类schaefer2018}_{数据来源neurosketch}_{最好的megaROI 包含有的数目}\n",
    "    di = load_obj(f\"./tmp__folder/{subject}_{N}_{roiloc}_{dataSource}_{bestID[subject]}\")\n",
    "    chosenMask[subject] = di['bestROIs']\n",
    "\n",
    "def getMask(topN, subject):\n",
    "    workingDir=\"/gpfs/milgram/project/turk-browne/projects/rtTest/\"\n",
    "    for pn, parc in enumerate(topN):\n",
    "        _mask = nib.load(workingDir+\"/{}/{}/{}\".format(roiloc, subject, parc))\n",
    "        aff = _mask.affine\n",
    "        _mask = _mask.get_data()\n",
    "        _mask = _mask.astype(int)\n",
    "        # say some things about the mask.\n",
    "        mask = _mask if pn == 0 else mask + _mask\n",
    "        mask[mask>0] = 1\n",
    "    return mask\n",
    "\n",
    "for sub in chosenMask:\n",
    "    mask=getMask(chosenMask[sub], sub)\n",
    "    # if not os.path.exists(f\"{workingDir}/{roiloc}/{sub}/chosenMask.npy\"):\n",
    "    np.save(f\"{workingDir}/{roiloc}/{sub}/chosenMask\",mask)\n",
    "    \n",
    "\n",
    "from scipy.stats import zscore\n",
    "def normalize(X):\n",
    "    _X=X.copy()\n",
    "    _X = zscore(_X, axis=0)\n",
    "    _X[np.isnan(_X)]=0\n",
    "    return _X\n",
    "\n",
    "def mkdir(folder):\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "\n",
    "\n",
    "'''\n",
    "load the functional and behavior data and choseMask and train all possible pairs of 2way classifiers\n",
    "''' \n",
    "def minimalClass(subject):\n",
    "    '''\n",
    "    purpose: \n",
    "        train offline models\n",
    "\n",
    "    steps:\n",
    "        load preprocessed and aligned behavior and brain data \n",
    "        select data with the wanted pattern like AB AC AD BC BD CD \n",
    "        train correspondng classifier and save the classifier performance and the classifiers themselves.\n",
    "\n",
    "    '''\n",
    "\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import sklearn\n",
    "    import joblib\n",
    "    import nibabel as nib\n",
    "    import itertools\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    def gaussian(x, mu, sig):\n",
    "        # mu and sig is determined before each neurofeedback session using 2 recognition runs.\n",
    "        return round(1+18*(1 - np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.))))) # map from (0,1) -> [1,19]\n",
    "\n",
    "    def jitter(size,const=0):\n",
    "        jit = np.random.normal(0+const, 0.05, size)\n",
    "        X = np.zeros((size))\n",
    "        X = X + jit\n",
    "        return X\n",
    "\n",
    "    def other(target):\n",
    "        other_objs = [i for i in ['bed', 'bench', 'chair', 'table'] if i not in target]\n",
    "        return other_objs\n",
    "\n",
    "    def red_vox(n_vox, prop=0.1):\n",
    "        return int(np.ceil(n_vox * prop))\n",
    "\n",
    "    def get_inds(X, Y, pair, testRun=None):\n",
    "\n",
    "        inds = {}\n",
    "\n",
    "        # return relative indices\n",
    "        if testRun:\n",
    "            trainIX = Y.index[(Y['label'].isin(pair)) & (Y['run_num'] != int(testRun))]\n",
    "        else:\n",
    "            trainIX = Y.index[(Y['label'].isin(pair))]\n",
    "\n",
    "        # pull training and test data\n",
    "        trainX = X[trainIX]\n",
    "        trainY = Y.iloc[trainIX].label\n",
    "\n",
    "        # Main classifier on 5 runs, testing on 6th\n",
    "        clf = LogisticRegression(penalty='l2',C=1, solver='lbfgs', max_iter=1000, \n",
    "                                multi_class='multinomial').fit(trainX, trainY)\n",
    "        B = clf.coef_[0]  # pull betas\n",
    "\n",
    "        # retrieve only the first object, then only the second object\n",
    "        if testRun:\n",
    "            obj1IX = Y.index[(Y['label'] == pair[0]) & (Y['run_num'] != int(testRun))]\n",
    "            obj2IX = Y.index[(Y['label'] == pair[1]) & (Y['run_num'] != int(testRun))]\n",
    "        else:\n",
    "            obj1IX = Y.index[(Y['label'] == pair[0])]\n",
    "            obj2IX = Y.index[(Y['label'] == pair[1])]\n",
    "\n",
    "        # Get the average of the first object, then the second object\n",
    "        obj1X = np.mean(X[obj1IX], 0)\n",
    "        obj2X = np.mean(X[obj2IX], 0)\n",
    "\n",
    "        # Build the importance map\n",
    "        mult1X = obj1X * B\n",
    "        mult2X = obj2X * B\n",
    "\n",
    "        # Sort these so that they are from least to most important for a given category.\n",
    "        sortmult1X = mult1X.argsort()[::-1]\n",
    "        sortmult2X = mult2X.argsort()\n",
    "\n",
    "        # add to a dictionary for later use\n",
    "        inds[clf.classes_[0]] = sortmult1X\n",
    "        inds[clf.classes_[1]] = sortmult2X\n",
    "\n",
    "        return inds\n",
    "\n",
    "    if 'milgram' in os.getcwd():\n",
    "        main_dir='/gpfs/milgram/project/turk-browne/projects/rtTest/'\n",
    "    else:\n",
    "        main_dir='/Users/kailong/Desktop/rtTest'\n",
    "\n",
    "    working_dir=main_dir\n",
    "    os.chdir(working_dir)\n",
    "\n",
    "    objects = ['bed', 'bench', 'chair', 'table']\n",
    "\n",
    "\n",
    "    if dataSource == \"neurosketch\":\n",
    "        funcdata = \"/gpfs/milgram/project/turk-browne/jukebox/ntb/projects/sketchloop02/subjects/{sub}_neurosketch/data/nifti/realtime_preprocessed/{sub}_neurosketch_recognition_run_{run}.nii.gz\"\n",
    "        metadata = \"/gpfs/milgram/project/turk-browne/jukebox/ntb/projects/sketchloop02/data/features/recog/metadata_{sub}_V1_{phase}.csv\"\n",
    "        anat = \"/gpfs/milgram/project/turk-browne/jukebox/ntb/projects/sketchloop02/subjects/{sub}_neurosketch/data/nifti/{sub}_neurosketch_anat_mprage_brain.nii.gz\"\n",
    "    elif dataSource == \"realtime\":\n",
    "        funcdata = \"/gpfs/milgram/project/turk-browne/projects/rtcloud_kp/subjects/{sub}/ses{ses}_recognition/run0{run}/nifti/{sub}_functional.nii.gz\"\n",
    "        metadata = \"/gpfs/milgram/project/turk-browne/projects/rtcloud_kp/subjects/{sub}/ses{ses}_recognition/run0{run}/{sub}_0{run}_preprocessed_behavData.csv\"\n",
    "        anat = \"$TO_BE_FILLED\"\n",
    "    else:\n",
    "        funcdata = \"/gpfs/milgram/project/turk-browne/projects/rtTest/searchout/feat/{sub}_pre.nii.gz\"\n",
    "        metadata = \"/gpfs/milgram/project/turk-browne/jukebox/ntb/projects/sketchloop02/data/features/recog/metadata_{sub}_V1_{phase}.csv\"\n",
    "        anat = \"$TO_BE_FILLED\"\n",
    "\n",
    "    # print('mask dimensions: {}'. format(mask.shape))\n",
    "    # print('number of voxels in mask: {}'.format(np.sum(mask)))\n",
    "    phasedict = dict(zip([1,2,3,4,5,6],[\"12\", \"12\", \"34\", \"34\", \"56\", \"56\"]))\n",
    "    imcodeDict={\"A\": \"bed\", \"B\": \"Chair\", \"C\": \"table\", \"D\": \"bench\"}\n",
    "    chosenMask = np.load(f\"/gpfs/milgram/project/turk-browne/projects/rtTest/schaefer2018/{subject}/chosenMask.npy\")\n",
    "    print(f\"np.sum(chosenMask)={np.sum(chosenMask)}\")\n",
    "    # Compile preprocessed data and corresponding indices\n",
    "    metas = []\n",
    "    for run in range(1, 7):\n",
    "        print(run, end='--')\n",
    "        # retrieve from the dictionary which phase it is, assign the session\n",
    "        phase = phasedict[run]\n",
    "        \n",
    "        # Build the path for the preprocessed functional data\n",
    "        this4d = funcdata.format(run=run, phase=phase, sub=subject)\n",
    "        \n",
    "        # Read in the metadata, and reduce it to only the TR values from this run, add to a list\n",
    "        thismeta = pd.read_csv(metadata.format(run=run, phase=phase, sub=subject))\n",
    "        if dataSource == \"neurosketch\":\n",
    "            _run = 1 if run % 2 == 0 else 2\n",
    "        else:\n",
    "            _run = run\n",
    "        thismeta = thismeta[thismeta['run_num'] == int(_run)]\n",
    "        \n",
    "        if dataSource == \"realtime\":\n",
    "            TR_num = list(thismeta.TR.astype(int))\n",
    "            labels = list(thismeta.Item)\n",
    "            labels = [imcodeDict[label] for label in labels]\n",
    "        else:\n",
    "            TR_num = list(thismeta.TR_num.astype(int))\n",
    "            labels = list(thismeta.label)\n",
    "        \n",
    "        print(\"LENGTH OF TR: {}\".format(len(TR_num)))\n",
    "        # Load the functional data\n",
    "        runIm = nib.load(this4d)\n",
    "        affine_mat = runIm.affine\n",
    "        runImDat = runIm.get_fdata()\n",
    "        \n",
    "        # Use the TR numbers to select the correct features\n",
    "        features = [runImDat[:,:,:,n+3] for n in TR_num] # here shape is from (94, 94, 72, 240) to (80, 94, 94, 72)\n",
    "        features = np.array(features)\n",
    "        features = features[:, chosenMask==1]\n",
    "        print(\"shape of features\", features.shape, \"shape of chosenMask\", chosenMask.shape)\n",
    "        features = normalize(features)\n",
    "        # features = np.expand_dims(features, 0)\n",
    "        \n",
    "        # Append both so we can use it later\n",
    "        # metas.append(labels)\n",
    "        # metas['label']\n",
    "\n",
    "        t=pd.DataFrame()\n",
    "        t['label']=labels\n",
    "        t[\"run_num\"]=run\n",
    "        behav_data=t if run==1 else pd.concat([behav_data,t])\n",
    "        \n",
    "        runs = features if run == 1 else np.concatenate((runs, features))\n",
    "\n",
    "    dimsize = runIm.header.get_zooms()\n",
    "    brain_data = runs\n",
    "    print(brain_data.shape)\n",
    "    print(behav_data.shape)\n",
    "    FEAT=brain_data\n",
    "    print(f\"FEAT.shape={FEAT.shape}\")\n",
    "    META=behav_data\n",
    "\n",
    "    def Class(brain_data,behav_data):\n",
    "        accs = []\n",
    "        for run in range(1,7):\n",
    "            trainIX = behav_data['run_num']!=int(run)\n",
    "            testIX = behav_data['run_num']==int(run)\n",
    "\n",
    "            trainX =  brain_data[trainIX]\n",
    "            trainY =  behav_data.iloc[np.asarray(trainIX)].label\n",
    "\n",
    "            testX =  brain_data[testIX]\n",
    "            testY =  behav_data.iloc[np.asarray(testIX)].label\n",
    "\n",
    "            clf = LogisticRegression(penalty='l2',C=1, solver='lbfgs', max_iter=1000, \n",
    "                                    multi_class='multinomial').fit(trainX, trainY)\n",
    "\n",
    "            # Monitor progress by printing accuracy (only useful if you're running a test set)\n",
    "            acc = clf.score(testX, testY)\n",
    "            accs.append(acc)\n",
    "        accs\n",
    "        return np.mean(accs)\n",
    "    accs=Class(brain_data,behav_data)\n",
    "    print(f\"new trained 4 way classifier accuracy={accs}\")\n",
    "\n",
    "\n",
    "    # convert item colume to label colume\n",
    "    imcodeDict={\n",
    "    'A': 'bed',\n",
    "    'B': 'chair',\n",
    "    'C': 'table',\n",
    "    'D': 'bench'}\n",
    "\n",
    "    # Which run to use as test data (leave as None to not have test data)\n",
    "    testRun = 6 # when testing: testRun = 2 ; META['run_num'].iloc[:5]=2\n",
    "\n",
    "    # Decide on the proportion of crescent data to use for classification\n",
    "    include = 1\n",
    "    objects = ['bed', 'bench', 'chair', 'table']\n",
    "    allpairs = itertools.combinations(objects,2)\n",
    "    accs={}\n",
    "    # Iterate over all the possible target pairs of objects\n",
    "    for pair in allpairs:\n",
    "        # Find the control (remaining) objects for this pair\n",
    "        altpair = other(pair)\n",
    "\n",
    "        # pull sorted indices for each of the critical objects, in order of importance (low to high)\n",
    "        # inds = get_inds(FEAT, META, pair, testRun=testRun)\n",
    "\n",
    "        # Find the number of voxels that will be left given your inclusion parameter above\n",
    "        # nvox = red_vox(FEAT.shape[1], include)\n",
    "\n",
    "        for obj in pair:\n",
    "            # foil = [i for i in pair if i != obj][0]\n",
    "            for altobj in altpair:\n",
    "\n",
    "                # establish a naming convention where it is $TARGET_$CLASSIFICATION\n",
    "                # Target is the NF pair (e.g. bed/bench)\n",
    "                # Classificationis is btw one of the targets, and a control (e.g. bed/chair, or bed/table, NOT bed/bench)\n",
    "                naming = '{}{}_{}{}'.format(pair[0], pair[1], obj, altobj)\n",
    "\n",
    "                # Pull the relevant inds from your previously established dictionary \n",
    "                # obj_inds = inds[obj]\n",
    "\n",
    "                # If you're using testdata, this function will split it up. Otherwise it leaves out run as a parameter\n",
    "                # if testRun:\n",
    "                #     trainIX = META.index[(META['label'].isin([obj, altobj])) & (META['run_num'] != int(testRun))]\n",
    "                #     testIX = META.index[(META['label'].isin([obj, altobj])) & (META['run_num'] == int(testRun))]\n",
    "                # else:\n",
    "                #     trainIX = META.index[(META['label'].isin([obj, altobj]))]\n",
    "                #     testIX = META.index[(META['label'].isin([obj, altobj]))]\n",
    "                # # pull training and test data\n",
    "                # trainX = FEAT[trainIX]\n",
    "                # testX = FEAT[testIX]\n",
    "                # trainY = META.iloc[trainIX].label\n",
    "                # testY = META.iloc[testIX].label\n",
    "\n",
    "                # print(f\"obj={obj},altobj={altobj}\")\n",
    "                # print(f\"unique(trainY)={np.unique(trainY)}\")\n",
    "                # print(f\"unique(testY)={np.unique(testY)}\")\n",
    "                # assert len(np.unique(trainY))==2\n",
    "\n",
    "                # for testRun in range(6):\n",
    "                if testRun:\n",
    "                    trainIX = ((META['label']==obj) + (META['label']==altobj)) * (META['run_num']!=int(testRun))\n",
    "                    testIX = ((META['label']==obj) + (META['label']==altobj)) * (META['run_num']==int(testRun))\n",
    "                else:\n",
    "                    trainIX = ((META['label']==obj) + (META['label']==altobj))\n",
    "                    testIX = ((META['label']==obj) + (META['label']==altobj))\n",
    "                # pull training and test data\n",
    "                trainX = FEAT[trainIX]\n",
    "                testX = FEAT[testIX]\n",
    "                trainY = META.iloc[np.asarray(trainIX)].label\n",
    "                testY = META.iloc[np.asarray(testIX)].label\n",
    "\n",
    "                # print(f\"obj={obj},altobj={altobj}\")\n",
    "                # print(f\"unique(trainY)={np.unique(trainY)}\")\n",
    "                # print(f\"unique(testY)={np.unique(testY)}\")\n",
    "                assert len(np.unique(trainY))==2\n",
    "\n",
    "                # # If you're selecting high-importance features, this bit handles that\n",
    "                # if include < 1:\n",
    "                #     trainX = trainX[:, obj_inds[-nvox:]]\n",
    "                #     testX = testX[:, obj_inds[-nvox:]]\n",
    "\n",
    "                # Train your classifier\n",
    "                clf = LogisticRegression(penalty='l2',C=1, solver='lbfgs', max_iter=1000, \n",
    "                                        multi_class='multinomial').fit(trainX, trainY)\n",
    "\n",
    "\n",
    "                model_folder = f\"{working_dir}{roiloc}/{subject}/clf/\"\n",
    "                mkdir(model_folder)\n",
    "                # Save it for later use\n",
    "                joblib.dump(clf, model_folder +'/{}.joblib'.format(naming))\n",
    "\n",
    "                # Monitor progress by printing accuracy (only useful if you're running a test set)\n",
    "                acc = clf.score(testX, testY)\n",
    "                # print(naming, acc)\n",
    "                accs[naming]=acc\n",
    "    \n",
    "    # _=plt.figure()\n",
    "    # _=plt.hist(list(accs.values()))\n",
    "    return accs \n",
    "\n",
    "\n",
    "'''\n",
    "calculate the evidence floor and ceil for each subject and display different forms of evidences.\n",
    "'''\n",
    "def morphingTarget(subject,testRun=6):\n",
    "    '''\n",
    "    purpose:\n",
    "        get the morphing target function\n",
    "    steps:\n",
    "        load train clf\n",
    "        load brain data and behavior data\n",
    "        get the morphing target function\n",
    "            evidence_floor is C evidence for CD classifier(can also be D evidence for CD classifier)\n",
    "            evidence_ceil  is A evidence in AC and AD classifier\n",
    "    '''\n",
    "\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    import nibabel as nib\n",
    "\n",
    "\n",
    "    phasedict = dict(zip([1,2,3,4,5,6],[\"12\", \"12\", \"34\", \"34\", \"56\", \"56\"]))\n",
    "    imcodeDict={\"A\": \"bed\", \"B\": \"Chair\", \"C\": \"table\", \"D\": \"bench\"}\n",
    "    if 'milgram' in os.getcwd():\n",
    "        main_dir='/gpfs/milgram/project/turk-browne/projects/rtTest/'\n",
    "    else:\n",
    "        main_dir='/Users/kailong/Desktop/rtTest'\n",
    "\n",
    "    working_dir=main_dir\n",
    "    os.chdir(working_dir)\n",
    "\n",
    "    funcdata = \"/gpfs/milgram/project/turk-browne/jukebox/ntb/projects/sketchloop02/subjects/{sub}_neurosketch/data/nifti/realtime_preprocessed/{sub}_neurosketch_recognition_run_{run}.nii.gz\"\n",
    "    metadata = \"/gpfs/milgram/project/turk-browne/jukebox/ntb/projects/sketchloop02/data/features/recog/metadata_{sub}_V1_{phase}.csv\"\n",
    "\n",
    "    metas = []\n",
    "    # for run in range(1, 7):\n",
    "    #     print(run, end='--')\n",
    "    #     # retrieve from the dictionary which phase it is, assign the session\n",
    "    #     phase = phasedict[run]\n",
    "    #     ses = 1\n",
    "        \n",
    "    #     # Build the path for the preprocessed functional data\n",
    "    #     this4d = funcdata.format(ses=ses, run=run, phase=phase, sub=subject)\n",
    "        \n",
    "    #     # Read in the metadata, and reduce it to only the TR values from this run, add to a list\n",
    "    #     thismeta = pd.read_csv(metadata.format(ses=ses, run=run, phase=phase, sub=subject))\n",
    "    #     if dataSource == \"neurosketch\":\n",
    "    #         _run = 1 if run % 2 == 0 else 2\n",
    "    #     else:\n",
    "    #         _run = run\n",
    "    #     thismeta = thismeta[thismeta['run_num'] == int(_run)]\n",
    "        \n",
    "    #     if dataSource == \"realtime\":\n",
    "    #         TR_num = list(thismeta.TR.astype(int))\n",
    "    #         labels = list(thismeta.Item)\n",
    "    #         labels = [imcodeDict[label] for label in labels]\n",
    "    #     else:\n",
    "    #         TR_num = list(thismeta.TR_num.astype(int))\n",
    "    #         labels = list(thismeta.label)\n",
    "        \n",
    "    #     print(\"LENGTH OF TR: {}\".format(len(TR_num)))\n",
    "    #     # Load the functional data\n",
    "    #     runIm = nib.load(this4d)\n",
    "    #     affine_mat = runIm.affine\n",
    "    #     runImDat = runIm.get_fdata()\n",
    "        \n",
    "    #     # Use the TR numbers to select the correct features\n",
    "    #     features = [runImDat[:,:,:,n+3] for n in TR_num]\n",
    "    #     features = np.array(features)\n",
    "    #     chosenMask = np.load(f\"/gpfs/milgram/project/turk-browne/projects/rtTest/schaefer2018/{subject}/chosenMask.npy\")\n",
    "    #     features = features[:, chosenMask==1]\n",
    "    #     print(\"shape of features\", features.shape, \"shape of mask\", mask.shape)\n",
    "    #     # featmean = features.mean(1).mean(1).mean(1)[..., None,None,None] #features.mean(1)[..., None]\n",
    "    #     # features = features - featmean\n",
    "    #     # features = features - features.mean(0)\n",
    "    #     features = normalize(features)\n",
    "    #     # features = np.expand_dims(features, 0)\n",
    "        \n",
    "    #     # Append both so we can use it later\n",
    "    #     # metas.append(labels)\n",
    "    #     # metas['label']\n",
    "\n",
    "    #     t=pd.DataFrame()\n",
    "    #     t['label']=labels\n",
    "    #     t[\"run_num\"]=run\n",
    "    #     behav_data=t if run==1 else pd.concat([behav_data,t])\n",
    "        \n",
    "    #     runs = features if run == 1 else np.concatenate((runs, features))\n",
    "    # for run in range(1, 7):\n",
    "    run=testRun\n",
    "    print(run, end='--')\n",
    "    # retrieve from the dictionary which phase it is, assign the session\n",
    "    phase = phasedict[run]\n",
    "    ses = 1\n",
    "    \n",
    "    # Build the path for the preprocessed functional data\n",
    "    this4d = funcdata.format(ses=ses, run=run, phase=phase, sub=subject)\n",
    "    \n",
    "    # Read in the metadata, and reduce it to only the TR values from this run, add to a list\n",
    "    thismeta = pd.read_csv(metadata.format(ses=ses, run=run, phase=phase, sub=subject))\n",
    "    if dataSource == \"neurosketch\":\n",
    "        _run = 1 if run % 2 == 0 else 2\n",
    "    else:\n",
    "        _run = run\n",
    "    thismeta = thismeta[thismeta['run_num'] == int(_run)]\n",
    "    \n",
    "    if dataSource == \"realtime\":\n",
    "        TR_num = list(thismeta.TR.astype(int))\n",
    "        labels = list(thismeta.Item)\n",
    "        labels = [imcodeDict[label] for label in labels]\n",
    "    else:\n",
    "        TR_num = list(thismeta.TR_num.astype(int))\n",
    "        labels = list(thismeta.label)\n",
    "    \n",
    "    print(\"LENGTH OF TR: {}\".format(len(TR_num)))\n",
    "    # Load the functional data\n",
    "    runIm = nib.load(this4d)\n",
    "    affine_mat = runIm.affine\n",
    "    runImDat = runIm.get_fdata()\n",
    "    \n",
    "    # Use the TR numbers to select the correct features\n",
    "    features = [runImDat[:,:,:,n+3] for n in TR_num]\n",
    "    features = np.array(features)\n",
    "    chosenMask = np.load(f\"/gpfs/milgram/project/turk-browne/projects/rtTest/schaefer2018/{subject}/chosenMask.npy\")\n",
    "    features = features[:, chosenMask==1]\n",
    "    print(\"shape of features\", features.shape, \"shape of mask\", mask.shape)\n",
    "    # featmean = features.mean(1).mean(1).mean(1)[..., None,None,None] #features.mean(1)[..., None]\n",
    "    # features = features - featmean\n",
    "    # features = features - features.mean(0)\n",
    "    features = normalize(features)\n",
    "    # features = np.expand_dims(features, 0)\n",
    "    \n",
    "    # Append both so we can use it later\n",
    "    # metas.append(labels)\n",
    "    # metas['label']\n",
    "\n",
    "    t=pd.DataFrame()\n",
    "    t['label']=labels\n",
    "    t[\"run_num\"]=run\n",
    "    behav_data=t\n",
    "    \n",
    "    runs = features\n",
    "\n",
    "    \n",
    "    dimsize = runIm.header.get_zooms()\n",
    "    \n",
    "    brain_data = runs\n",
    "    print(brain_data.shape)\n",
    "    print(behav_data.shape)\n",
    "    FEAT=brain_data\n",
    "    print(f\"FEAT.shape={FEAT.shape}\")\n",
    "    META=behav_data\n",
    "\n",
    "    # print('mask dimensions: {}'. format(mask.shape))\n",
    "    # print('number of voxels in mask: {}'.format(np.sum(mask)))\n",
    "\n",
    "    # runRecording = pd.read_csv(f\"{cfg.recognition_dir}../runRecording.csv\")\n",
    "    # actualRuns = list(runRecording['run'].iloc[list(np.where(1==1*(runRecording['type']=='recognition'))[0])]) # can be [1,2,3,4,5,6,7,8] or [1,2,4,5]\n",
    "\n",
    "    # objects = ['bed', 'bench', 'chair', 'table']\n",
    "\n",
    "    # for ii,run in enumerate(actualRuns[:2]): # load behavior and brain data for current session\n",
    "    #     t = np.load(f\"{cfg.recognition_dir}brain_run{run}.npy\")\n",
    "    #     # mask = nib.load(f\"{cfg.chosenMask}\").get_data()\n",
    "    #     mask = np.load(cfg.chosenMask)\n",
    "    #     t = t[:,mask==1]\n",
    "    #     t = normalize(t)\n",
    "    #     brain_data=t if ii==0 else np.concatenate((brain_data,t), axis=0)\n",
    "\n",
    "    #     t = pd.read_csv(f\"{cfg.recognition_dir}behav_run{run}.csv\")\n",
    "    #     behav_data=t if ii==0 else pd.concat([behav_data,t])\n",
    "\n",
    "    # FEAT=brain_data.reshape(brain_data.shape[0],-1)\n",
    "    # # FEAT_mean=np.mean(FEAT,axis=1)\n",
    "    # # FEAT=(FEAT.T-FEAT_mean).T\n",
    "    # # FEAT_mean=np.mean(FEAT,axis=0)\n",
    "    # # FEAT=FEAT-FEAT_mean\n",
    "\n",
    "    # META=behav_data\n",
    "\n",
    "    # convert item colume to label colume\n",
    "    imcodeDict={\n",
    "    'A': 'bed',\n",
    "    'B': 'chair',\n",
    "    'C': 'table',\n",
    "    'D': 'bench'}\n",
    "    # label=[]\n",
    "    # for curr_trial in range(META.shape[0]):\n",
    "    #     label.append(imcodeDict[META['Item'].iloc[curr_trial]])\n",
    "    # META['label']=label # merge the label column with the data dataframe\n",
    "\n",
    "\n",
    "    # def classifierEvidence(clf,X,Y): # X shape is [trials,voxelNumber], Y is ['bed', 'bed'] for example # return a 1-d array of probability\n",
    "    #     # This function get the data X and evidence object I want to know Y, and output the trained model evidence.\n",
    "    #     targetID=[np.where((clf.classes_==i)==True)[0][0] for i in Y]\n",
    "    #     # Evidence=(np.sum(X*clf.coef_,axis=1)+clf.intercept_) if targetID[0]==1 else (1-(np.sum(X*clf.coef_,axis=1)+clf.intercept_))\n",
    "    #     Evidence=(X@clf.coef_.T+clf.intercept_) if targetID[0]==1 else (-(X@clf.coef_.T+clf.intercept_))\n",
    "    #     Evidence = 1/(1+np.exp(-Evidence))\n",
    "    #     return np.asarray(Evidence)\n",
    "\n",
    "    # def classifierEvidence(clf,X,Y):\n",
    "    #     ID=np.where((clf.classes_==Y[0])*1==1)[0][0]\n",
    "    #     p = clf.predict_proba(X)[:,ID]\n",
    "    #     BX=np.log(p/(1-p))\n",
    "    #     return BX\n",
    "\n",
    "    def classifierEvidence(clf,X,Y):\n",
    "        ID=np.where((clf.classes_==Y[0])*1==1)[0][0]\n",
    "        Evidence=(X@clf.coef_.T+clf.intercept_) if ID==1 else (-(X@clf.coef_.T+clf.intercept_))\n",
    "        # Evidence=(X@clf.coef_.T+clf.intercept_) if ID==0 else (-(X@clf.coef_.T+clf.intercept_))\n",
    "        return np.asarray(Evidence)\n",
    "\n",
    "    A_ID = (META['label']=='bed')\n",
    "    X = FEAT[A_ID]\n",
    "\n",
    "    # evidence_floor is C evidence for AC_CD BC_CD CD_CD classifier(can also be D evidence for CD classifier)\n",
    "    # Y = ['table'] * X.shape[0]\n",
    "    # CD_clf=joblib.load(cfg.usingModel_dir +'bedbench_benchtable.joblib') # These 4 clf are the same: bedbench_benchtable.joblib bedtable_tablebench.joblib benchchair_benchtable.joblib chairtable_tablebench.joblib\n",
    "    # CD_C_evidence = classifierEvidence(CD_clf,X,Y)\n",
    "    # evidence_floor = np.mean(CD_C_evidence)\n",
    "    # print(f\"evidence_floor={evidence_floor}\")\n",
    "\n",
    "    model_folder = f\"{working_dir}{roiloc}/{subject}/clf/\"\n",
    "\n",
    "    # #try out other forms of floor: C evidence in AC and D evidence for AD\n",
    "    # Y = ['bench'] * X.shape[0]\n",
    "    # AD_clf=joblib.load(model_folder +'bedchair_bedbench.joblib') # These 4 clf are the same:   bedchair_bedbench.joblib bedtable_bedbench.joblib benchchair_benchbed.joblib benchtable_benchbed.joblib\n",
    "    # AD_D_evidence = classifierEvidence(AD_clf,X,Y)\n",
    "    # evidence_floor = np.mean(AD_D_evidence)\n",
    "    # print(f\"evidence_floor2={np.mean(evidence_floor)}\")\n",
    "\n",
    "\n",
    "\n",
    "    # # floor\n",
    "    # Y = ['bench'] * X.shape[0]\n",
    "    # CD_clf=joblib.load(model_folder +'bedbench_benchtable.joblib') # These 4 clf are the same: bedbench_benchtable.joblib bedtable_tablebench.joblib benchchair_benchtable.joblib chairtable_tablebench.joblib\n",
    "    # CD_D_evidence = classifierEvidence(CD_clf,X,Y)\n",
    "    # evidence_floor = np.mean(CD_D_evidence)\n",
    "    # print(f\"evidence_floor={evidence_floor}\")\n",
    "\n",
    "    # Y = ['table'] * X.shape[0]\n",
    "    # CD_clf=joblib.load(model_folder +'bedbench_benchtable.joblib') # These 4 clf are the same: bedbench_benchtable.joblib bedtable_tablebench.joblib benchchair_benchtable.joblib chairtable_tablebench.joblib\n",
    "    # CD_C_evidence = classifierEvidence(CD_clf,X,Y)\n",
    "    # evidence_floor = np.mean(CD_C_evidence)\n",
    "    # print(f\"evidence_floor={evidence_floor}\")\n",
    "\n",
    "\n",
    "    # # evidence_ceil  is A evidence in AC and AD classifier\n",
    "    # Y = ['bed'] * X.shape[0]\n",
    "    # AC_clf=joblib.load(model_folder +'benchtable_tablebed.joblib') # These 4 clf are the same:   bedbench_bedtable.joblib bedchair_bedtable.joblib benchtable_tablebed.joblib chairtable_tablebed.joblib\n",
    "    # AC_A_evidence = classifierEvidence(AC_clf,X,Y)\n",
    "    # evidence_ceil1 = AC_A_evidence\n",
    "    # print(f\"evidence_ceil1={np.mean(evidence_ceil1)}\")\n",
    "\n",
    "    # Y = ['bed'] * X.shape[0]\n",
    "    # AD_clf=joblib.load(model_folder +'bedchair_bedbench.joblib') # These 4 clf are the same:   bedchair_bedbench.joblib bedtable_bedbench.joblib benchchair_benchbed.joblib benchtable_benchbed.joblib\n",
    "    # AD_A_evidence = classifierEvidence(AD_clf,X,Y)\n",
    "    # evidence_ceil2 = AD_A_evidence\n",
    "    # print(f\"evidence_ceil2={np.mean(evidence_ceil2)}\")\n",
    "\n",
    "    # # evidence_ceil = np.mean(evidence_ceil1)\n",
    "    # # evidence_ceil = np.mean(evidence_ceil2)\n",
    "    # evidence_ceil = np.mean((evidence_ceil1+evidence_ceil2)/2)\n",
    "    # print(f\"evidence_ceil={evidence_ceil}\")\n",
    "    store=\"\\n\"\n",
    "    print(\"floor\")\n",
    "    # D evidence for AD_clf when A is presented.\n",
    "    Y = ['bench'] * X.shape[0]\n",
    "    AD_clf=joblib.load(model_folder +'bedchair_bedbench.joblib') # These 4 clf are the same:   bedchair_bedbench.joblib bedtable_bedbench.joblib benchchair_benchbed.joblib benchtable_benchbed.joblib\n",
    "    AD_D_evidence = classifierEvidence(AD_clf,X,Y)\n",
    "    evidence_floor = np.mean(AD_D_evidence)\n",
    "    print(f\"D evidence for AD_clf when A is presented={evidence_floor}\")\n",
    "    store=store+f\"D evidence for AD_clf when A is presented={evidence_floor}\"\n",
    "\n",
    "    # C evidence for AC_clf when A is presented.\n",
    "    Y = ['table'] * X.shape[0]\n",
    "    AC_clf=joblib.load(model_folder +'benchtable_tablebed.joblib') # These 4 clf are the same:   bedbench_bedtable.joblib bedchair_bedtable.joblib benchtable_tablebed.joblib chairtable_tablebed.joblib\n",
    "    AC_C_evidence = classifierEvidence(AC_clf,X,Y)\n",
    "    evidence_floor = np.mean(AC_C_evidence)\n",
    "    print(f\"C evidence for AC_clf when A is presented={evidence_floor}\")\n",
    "    store=store+\"\\n\"+f\"C evidence for AC_clf when A is presented={evidence_floor}\"\n",
    "\n",
    "    # D evidence for CD_clf when A is presented.\n",
    "    Y = ['bench'] * X.shape[0]\n",
    "    CD_clf=joblib.load(model_folder +'bedbench_benchtable.joblib') # These 4 clf are the same: bedbench_benchtable.joblib bedtable_tablebench.joblib benchchair_benchtable.joblib chairtable_tablebench.joblib\n",
    "    CD_D_evidence = classifierEvidence(CD_clf,X,Y)\n",
    "    evidence_floor = np.mean(CD_D_evidence)\n",
    "    print(f\"D evidence for CD_clf when A is presented={evidence_floor}\")\n",
    "    store=store+\"\\n\"+f\"D evidence for CD_clf when A is presented={evidence_floor}\"\n",
    "\n",
    "    # C evidence for CD_clf when A is presented.\n",
    "    Y = ['table'] * X.shape[0]\n",
    "    CD_clf=joblib.load(model_folder +'bedbench_benchtable.joblib') # These 4 clf are the same: bedbench_benchtable.joblib bedtable_tablebench.joblib benchchair_benchtable.joblib chairtable_tablebench.joblib\n",
    "    CD_C_evidence = classifierEvidence(CD_clf,X,Y)\n",
    "    evidence_floor = np.mean(CD_C_evidence)\n",
    "    print(f\"C evidence for CD_clf when A is presented={evidence_floor}\")\n",
    "    store=store+\"\\n\"+f\"C evidence for CD_clf when A is presented={evidence_floor}\"\n",
    "\n",
    "\n",
    "    print(\"ceil\")\n",
    "    store=store+\"\\n\"+\"ceil\"\n",
    "    # evidence_ceil  is A evidence in AC and AD classifier\n",
    "    Y = ['bed'] * X.shape[0]\n",
    "    AC_clf=joblib.load(model_folder +'benchtable_tablebed.joblib') # These 4 clf are the same:   bedbench_bedtable.joblib bedchair_bedtable.joblib benchtable_tablebed.joblib chairtable_tablebed.joblib\n",
    "    AC_A_evidence = classifierEvidence(AC_clf,X,Y)\n",
    "    evidence_ceil1 = AC_A_evidence\n",
    "    print(f\"A evidence in AC_clf when A is presented={np.mean(evidence_ceil1)}\")\n",
    "    store=store+\"\\n\"+f\"A evidence in AC_clf when A is presented={np.mean(evidence_ceil1)}\"\n",
    "\n",
    "    Y = ['bed'] * X.shape[0]\n",
    "    AD_clf=joblib.load(model_folder +'bedchair_bedbench.joblib') # These 4 clf are the same:   bedchair_bedbench.joblib bedtable_bedbench.joblib benchchair_benchbed.joblib benchtable_benchbed.joblib\n",
    "    AD_A_evidence = classifierEvidence(AD_clf,X,Y)\n",
    "    evidence_ceil2 = AD_A_evidence\n",
    "    print(f\"A evidence in AD_clf when A is presented={np.mean(evidence_ceil2)}\")\n",
    "    store=store+\"\\n\"+f\"A evidence in AD_clf when A is presented={np.mean(evidence_ceil2)}\"\n",
    "\n",
    "    # evidence_ceil = np.mean(evidence_ceil1)\n",
    "    # evidence_ceil = np.mean(evidence_ceil2)\n",
    "    evidence_ceil = np.mean((evidence_ceil1+evidence_ceil2)/2)\n",
    "    print(f\"evidence_ceil={evidence_ceil}\")\n",
    "    store=store+\"\\n\"+f\"evidence_ceil={evidence_ceil}\"\n",
    "    ceil,floor=evidence_ceil,evidence_floor\n",
    "    mu = (ceil+floor)/2\n",
    "    sig = (ceil-floor)/2.3548\n",
    "    print(f\"floor={floor}, ceil={ceil}\")\n",
    "    print(f\"mu={mu}, sig={sig}\")\n",
    "\n",
    "    store=store+\"\\n\"+f\"floor={floor}, ceil={ceil}\"\n",
    "    store=store+\"\\n\"+f\"mu={mu}, sig={sig}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    BC_clf=joblib.load(model_folder +'benchchair_chairtable.joblib') # These 4 clf are the same: bedbench_benchtable.joblib bedtable_tablebench.joblib benchchair_benchtable.joblib chairtable_tablebench.joblib\n",
    "    BD_clf=joblib.load(model_folder +'bedchair_chairbench.joblib') # These 4 clf are the same: bedbench_benchtable.joblib bedtable_tablebench.joblib benchchair_benchtable.joblib chairtable_tablebench.joblib\n",
    "    Y = ['chair']*FEAT.shape[0]\n",
    "    # imcodeDict={\n",
    "    # 'A': 'bed',\n",
    "    # 'B': 'chair',\n",
    "    # 'C': 'table',\n",
    "    # 'D': 'bench'}\n",
    "    print(f\"classifierEvidence(BC_clf,FEAT,Y)={classifierEvidence(BC_clf,FEAT,Y)}\")\n",
    "    print(f\"classifierEvidence(BD_clf,FEAT,Y)={classifierEvidence(BD_clf,FEAT,Y)}\")\n",
    "    BC_B_evidence = classifierEvidence(BC_clf,X,Y)\n",
    "    BD_B_evidence = classifierEvidence(BD_clf,X,Y)\n",
    "    print(f\"BC_B_evidence={BC_B_evidence}\")\n",
    "    print(f\"BD_B_evidence={BD_B_evidence}\")\n",
    "    B_evidence = (BC_B_evidence+BD_B_evidence)/2\n",
    "    print(f\"B_evidence={B_evidence}\")\n",
    "    store=store+\"\\n\"+B_evidence\n",
    "    print(f\"mu={mu}, sig={sig}\")\n",
    "    def gaussian(x, mu, sig):\n",
    "        # mu and sig is determined before each neurofeedback session using 2 recognition runs.\n",
    "        return round(1+18*(1 - np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.))))) # map from (0,1) -> [1,19]\n",
    "    morphParam=int(gaussian(B_evidence, mu, sig))\n",
    "    # B_evidences.append(B_evidence)\n",
    "    print(f\"morphParam={morphParam}\")\n",
    "\n",
    "    return evidence_floor, evidence_ceil,store\n",
    "    \n",
    "    \n",
    "# sub_id=7\n",
    "import sys\n",
    "\n",
    "# subject= '0119173' #sys.argv[1]\n",
    "# sub_id = [i for i,x in enumerate(subjects) if x == subject][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def subLoop(subject):\n",
    "    data={}\n",
    "    accs = minimalClass(subject)\n",
    "    print(\"best 4way classifier accuracy = \",GreedyBestAcc[subject][bestID[subject]])\n",
    "    data['best 4way classifier accuracy']=GreedyBestAcc[subject][bestID[subject]]\n",
    "    for acc in accs:\n",
    "        print(acc,accs[acc])\n",
    "    data[\"accs\"]=accs\n",
    "    floor, ceil,store = morphingTarget(subject,testRun=6)\n",
    "    data[\"store testing run\"]=store\n",
    "    floor, ceil,store = morphingTarget(subject,testRun=1)\n",
    "    data[\"store training run\"]=store\n",
    "    \n",
    "    save_obj(store,f\"./{subject}store\")\n",
    "    return data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "data={}\n",
    "for subject in tqdm(subjects):\n",
    "    data[subject]=subLoop(subject)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # floorCeilNeurosketch_child.sh\n",
    "# #!/usr/bin/env bash\n",
    "# # Input python command to be submitted as a job\n",
    "# #SBATCH --output=logs/floorCeil-%j.out\n",
    "# #SBATCH --job-name floorCeil\n",
    "# #SBATCH --partition=short,day,scavenge,verylong\n",
    "# #SBATCH --time=1:00:00 #20:00:00\n",
    "# #SBATCH --mem=10000\n",
    "# #SBATCH -n 5\n",
    "\n",
    "# # Set up the environment\n",
    "\n",
    "# subject=$1\n",
    "\n",
    "# echo source activate /gpfs/milgram/project/turk-browne/users/kp578/CONDA/rtcloud\n",
    "# source activate /gpfs/milgram/project/turk-browne/users/kp578/CONDA/rtcloud\n",
    "\n",
    "# python -u ./floorCeilNeurosketch.py $subject\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # floorCeilNeurosketch_parent.sh\n",
    "# subjects=\"1206161 0119173 1206162 1130161 1206163 0120171 0111171 1202161 0125172 0110172 0123173 0120173 0110171 0119172 0124171 0123171 1203161 0118172 0118171 0112171 1207162 0117171 0119174 0112173 0112172\" #these subjects are done with the batchRegions code\n",
    "# for sub in $subjects\n",
    "# do\n",
    "#   for num in 25; #best ID is 30 thus the best num is 31\n",
    "#   do\n",
    "#     echo sbatch --requeue floorCeilNeurosketch_child.sh $sub\n",
    "#     sbatch --requeue floorCeilNeurosketch_child.sh $sub\n",
    "#   done\n",
    "# done\n",
    "'''\n",
    "这个code的目的是用neurosketch 的数据来检测现在在realtime data里面发现的issue：也就是ceiling有时候竟然比floor更小\n",
    "这个code的运行逻辑是\n",
    "用neurosketch前五个run训练2 way classifiers，然后用最后一个run来计算ceiling和floor的值，看是否合理\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "purpose:\n",
    "    find the best performed mask from the result of aggregate_greedy.py and save as chosenMask\n",
    "    train all possible pairs of 2way classifiers and save for evidence calculation\n",
    "    load saved classifiers and calculate different forms of evidence\n",
    "steps:\n",
    "    load the result of aggregate_greedy.py\n",
    "    display the result of aggregate_greedy.py\n",
    "    find the best performed ROI for each subject and display the accuracy of each subject, save the best performed ROI as chosenMask\n",
    "    load the functional and behavior data and choseMask and train all possible pairs of 2way classifiers\n",
    "    calculate the evidence floor and ceil for each subject and display different forms of evidences.\n",
    "    \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "load the result of aggregate_greedy.py\n",
    "'''\n",
    "# To visualize the greedy result starting for 31 ROIs, in total 25 subjects.\n",
    "import os\n",
    "os.chdir(\"/gpfs/milgram/project/turk-browne/projects/rtTest/kp_scratch/\")\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle5 as pickle\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import os\n",
    "print(f\"conda env={os.environ['CONDA_DEFAULT_ENV']}\") \n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import itertools\n",
    "import pickle\n",
    "import subprocess\n",
    "from subprocess import call\n",
    "workingDir=\"/gpfs/milgram/project/turk-browne/projects/rtTest/\"\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "roiloc=\"schaefer2018\"\n",
    "dataSource=\"neurosketch\"\n",
    "subjects_correctly_aligned=['1206161','0119173','1206162','1130161','1206163','0120171','0111171','1202161','0125172','0110172','0123173','0120173','0110171','0119172','0124171','0123171','1203161','0118172','0118171','0112171','1207162','0117171','0119174','0112173','0112172']\n",
    "subjects=subjects_correctly_aligned\n",
    "N=25\n",
    "workingPath=\"/gpfs/milgram/project/turk-browne/projects/rtTest/\"\n",
    "GreedyBestAcc=np.zeros((len(subjects),N+1))\n",
    "GreedyBestAcc[GreedyBestAcc==0]=None\n",
    "GreedyBestAcc={}\n",
    "numberOfROIs={}\n",
    "for ii,subject in enumerate(subjects):\n",
    "    # try:\n",
    "    #     GreedyBestAcc[ii,N]=np.load(workingPath+\"./{}/{}/output/uniMaskRanktag2_top{}.npy\".format(roiloc, subject, N))\n",
    "    # except:\n",
    "    #     pass\n",
    "    t=np.load(workingPath+\"./{}/{}/output/uniMaskRanktag2_top{}.npy\".format(roiloc, subject, N))\n",
    "    GreedyBestAcc[subject]=[np.float(t)]\n",
    "    numberOfROIs[subject]=[N]\n",
    "    # for len_topN_1 in range(N-1,0,-1):\n",
    "    for len_topN in range(1,N):\n",
    "        # Wait(f\"./tmp/{subject}_{N}_{roiloc}_{dataSource}_{len_topN_1}.pkl\")\n",
    "        try:\n",
    "            # {当前的被试}_{greedy开始的ROI数目，也就是25}_{mask的种类schaefer2018}_{数据来源neurosketch}_{当前的 megaROI 包含有的数目}\n",
    "            di = load_obj(f\"./tmp__folder/{subject}_{N}_{roiloc}_{dataSource}_{len_topN}\")\n",
    "            GreedyBestAcc[subject].append(np.float(di['bestAcc']))\n",
    "            numberOfROIs[subject].append(len_topN)\n",
    "            # GreedyBestAcc[ii,len_topN] = di['bestAcc']\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "# '''\n",
    "# to load the imtermediate results from greedy code to examine the system\n",
    "# '''\n",
    "# def wait(tmpFile):\n",
    "#     while not os.path.exists(tmpFile+'_result.npy'):\n",
    "#         time.sleep(5)\n",
    "#         print(f\"waiting for {tmpFile}_result.npy\\n\")\n",
    "#     return np.load(tmpFile+'_result.npy')\n",
    "\n",
    "# subject= '0119173' #sys.argv[1]\n",
    "# sub_id = [i for i,x in enumerate(subjects) if x == subject][0]\n",
    "# intermediate_result=np.zeros((N+1,N+1))\n",
    "# # 应该有多少？25个24ROI，2个1ROI，24个\n",
    "# for i in range(N,1,-1):\n",
    "#     for j in range(i):\n",
    "#         tmpFile=f\"./tmp__folder/{subject}_{N}_{roiloc}_{dataSource}_{i}_{j}\"\n",
    "#         sl_result=wait(tmpFile)\n",
    "#         intermediate_result[i,j]=sl_result\n",
    "\n",
    "# # _=plt.imshow(intermediate_result)\n",
    "# #最后一行是25个24ROI，第2行是2个1ROI\n",
    "\n",
    "'''\n",
    "display the result of aggregate_greedy.py\n",
    "'''\n",
    "# GreedyBestAcc=GreedyBestAcc.T\n",
    "# plt.imshow(GreedyBestAcc)\n",
    "# _=plt.figure()\n",
    "# for i in range(GreedyBestAcc.shape[0]):\n",
    "#     plt.scatter([i]*GreedyBestAcc.shape[1],GreedyBestAcc[i,:],c='g',s=2)\n",
    "# plt.plot(np.arange(GreedyBestAcc.shape[0]),np.nanmean(GreedyBestAcc,axis=1))\n",
    "# # plt.ylim([0.19,0.36])\n",
    "# # plt.xlabel(\"number of ROIs\")\n",
    "# # plt.ylabel(\"accuracy\")\n",
    "# _=plt.figure()\n",
    "# for j in range(GreedyBestAcc.shape[1]):\n",
    "#     plt.plot(GreedyBestAcc[:,j])\n",
    "\n",
    "\n",
    "# GreedyBestAcc=GreedyBestAcc.T\n",
    "# _=plt.figure()\n",
    "# plt.imshow(GreedyBestAcc)\n",
    "\n",
    "'''\n",
    "find the best performed ROI for each subject and display the accuracy of each subject, save the best performed ROI as chosenMask\n",
    "'''\n",
    "#find best ID for each subject\n",
    "bestID={}\n",
    "for ii,subject in enumerate(subjects):\n",
    "    t=GreedyBestAcc[subject]\n",
    "    bestID[subject] = numberOfROIs[subject][np.where(t==np.nanmax(t))[0][0]] #bestID 指的是每一个subject对应的最好的megaROI包含的ROI的数目\n",
    "chosenMask={}\n",
    "for subject in bestID:\n",
    "    # best ID  \n",
    "    # {当前的被试}_{greedy开始的ROI数目，也就是25}_{mask的种类schaefer2018}_{数据来源neurosketch}_{最好的megaROI 包含有的数目}\n",
    "    di = load_obj(f\"./tmp__folder/{subject}_{N}_{roiloc}_{dataSource}_{bestID[subject]}\")\n",
    "    chosenMask[subject] = di['bestROIs']\n",
    "\n",
    "def getMask(topN, subject):\n",
    "    workingDir=\"/gpfs/milgram/project/turk-browne/projects/rtTest/\"\n",
    "    for pn, parc in enumerate(topN):\n",
    "        _mask = nib.load(workingDir+\"/{}/{}/{}\".format(roiloc, subject, parc))\n",
    "        aff = _mask.affine\n",
    "        _mask = _mask.get_data()\n",
    "        _mask = _mask.astype(int)\n",
    "        # say some things about the mask.\n",
    "        mask = _mask if pn == 0 else mask + _mask\n",
    "        mask[mask>0] = 1\n",
    "    return mask\n",
    "\n",
    "for sub in chosenMask:\n",
    "    mask=getMask(chosenMask[sub], sub)\n",
    "    # if not os.path.exists(f\"{workingDir}/{roiloc}/{sub}/chosenMask.npy\"):\n",
    "    np.save(f\"{workingDir}/{roiloc}/{sub}/chosenMask\",mask)\n",
    "    \n",
    "\n",
    "from scipy.stats import zscore\n",
    "def normalize(X):\n",
    "    _X=X.copy()\n",
    "    _X = zscore(_X, axis=0)\n",
    "    _X[np.isnan(_X)]=0\n",
    "    return _X\n",
    "\n",
    "def mkdir(folder):\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "\n",
    "\n",
    "'''\n",
    "load the functional and behavior data and choseMask and train all possible pairs of 2way classifiers\n",
    "''' \n",
    "def minimalClass(subject):\n",
    "    '''\n",
    "    purpose: \n",
    "        train offline models\n",
    "\n",
    "    steps:\n",
    "        load preprocessed and aligned behavior and brain data \n",
    "        select data with the wanted pattern like AB AC AD BC BD CD \n",
    "        train correspondng classifier and save the classifier performance and the classifiers themselves.\n",
    "\n",
    "    '''\n",
    "\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import sklearn\n",
    "    import joblib\n",
    "    import nibabel as nib\n",
    "    import itertools\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    def gaussian(x, mu, sig):\n",
    "        # mu and sig is determined before each neurofeedback session using 2 recognition runs.\n",
    "        return round(1+18*(1 - np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.))))) # map from (0,1) -> [1,19]\n",
    "\n",
    "    def jitter(size,const=0):\n",
    "        jit = np.random.normal(0+const, 0.05, size)\n",
    "        X = np.zeros((size))\n",
    "        X = X + jit\n",
    "        return X\n",
    "\n",
    "    def other(target):\n",
    "        other_objs = [i for i in ['bed', 'bench', 'chair', 'table'] if i not in target]\n",
    "        return other_objs\n",
    "\n",
    "    def red_vox(n_vox, prop=0.1):\n",
    "        return int(np.ceil(n_vox * prop))\n",
    "\n",
    "    def get_inds(X, Y, pair, testRun=None):\n",
    "\n",
    "        inds = {}\n",
    "\n",
    "        # return relative indices\n",
    "        if testRun:\n",
    "            trainIX = Y.index[(Y['label'].isin(pair)) & (Y['run_num'] != int(testRun))]\n",
    "        else:\n",
    "            trainIX = Y.index[(Y['label'].isin(pair))]\n",
    "\n",
    "        # pull training and test data\n",
    "        trainX = X[trainIX]\n",
    "        trainY = Y.iloc[trainIX].label\n",
    "\n",
    "        # Main classifier on 5 runs, testing on 6th\n",
    "        clf = LogisticRegression(penalty='l2',C=1, solver='lbfgs', max_iter=1000, \n",
    "                                multi_class='multinomial').fit(trainX, trainY)\n",
    "        B = clf.coef_[0]  # pull betas\n",
    "\n",
    "        # retrieve only the first object, then only the second object\n",
    "        if testRun:\n",
    "            obj1IX = Y.index[(Y['label'] == pair[0]) & (Y['run_num'] != int(testRun))]\n",
    "            obj2IX = Y.index[(Y['label'] == pair[1]) & (Y['run_num'] != int(testRun))]\n",
    "        else:\n",
    "            obj1IX = Y.index[(Y['label'] == pair[0])]\n",
    "            obj2IX = Y.index[(Y['label'] == pair[1])]\n",
    "\n",
    "        # Get the average of the first object, then the second object\n",
    "        obj1X = np.mean(X[obj1IX], 0)\n",
    "        obj2X = np.mean(X[obj2IX], 0)\n",
    "\n",
    "        # Build the importance map\n",
    "        mult1X = obj1X * B\n",
    "        mult2X = obj2X * B\n",
    "\n",
    "        # Sort these so that they are from least to most important for a given category.\n",
    "        sortmult1X = mult1X.argsort()[::-1]\n",
    "        sortmult2X = mult2X.argsort()\n",
    "\n",
    "        # add to a dictionary for later use\n",
    "        inds[clf.classes_[0]] = sortmult1X\n",
    "        inds[clf.classes_[1]] = sortmult2X\n",
    "\n",
    "        return inds\n",
    "\n",
    "    if 'milgram' in os.getcwd():\n",
    "        main_dir='/gpfs/milgram/project/turk-browne/projects/rtTest/'\n",
    "    else:\n",
    "        main_dir='/Users/kailong/Desktop/rtTest'\n",
    "\n",
    "    working_dir=main_dir\n",
    "    os.chdir(working_dir)\n",
    "\n",
    "    objects = ['bed', 'bench', 'chair', 'table']\n",
    "\n",
    "\n",
    "    if dataSource == \"neurosketch\":\n",
    "        funcdata = \"/gpfs/milgram/project/turk-browne/jukebox/ntb/projects/sketchloop02/subjects/{sub}_neurosketch/data/nifti/realtime_preprocessed/{sub}_neurosketch_recognition_run_{run}.nii.gz\"\n",
    "        metadata = \"/gpfs/milgram/project/turk-browne/jukebox/ntb/projects/sketchloop02/data/features/recog/metadata_{sub}_V1_{phase}.csv\"\n",
    "        anat = \"/gpfs/milgram/project/turk-browne/jukebox/ntb/projects/sketchloop02/subjects/{sub}_neurosketch/data/nifti/{sub}_neurosketch_anat_mprage_brain.nii.gz\"\n",
    "    elif dataSource == \"realtime\":\n",
    "        funcdata = \"/gpfs/milgram/project/turk-browne/projects/rtcloud_kp/subjects/{sub}/ses{ses}_recognition/run0{run}/nifti/{sub}_functional.nii.gz\"\n",
    "        metadata = \"/gpfs/milgram/project/turk-browne/projects/rtcloud_kp/subjects/{sub}/ses{ses}_recognition/run0{run}/{sub}_0{run}_preprocessed_behavData.csv\"\n",
    "        anat = \"$TO_BE_FILLED\"\n",
    "    else:\n",
    "        funcdata = \"/gpfs/milgram/project/turk-browne/projects/rtTest/searchout/feat/{sub}_pre.nii.gz\"\n",
    "        metadata = \"/gpfs/milgram/project/turk-browne/jukebox/ntb/projects/sketchloop02/data/features/recog/metadata_{sub}_V1_{phase}.csv\"\n",
    "        anat = \"$TO_BE_FILLED\"\n",
    "\n",
    "    # print('mask dimensions: {}'. format(mask.shape))\n",
    "    # print('number of voxels in mask: {}'.format(np.sum(mask)))\n",
    "    phasedict = dict(zip([1,2,3,4,5,6],[\"12\", \"12\", \"34\", \"34\", \"56\", \"56\"]))\n",
    "    imcodeDict={\"A\": \"bed\", \"B\": \"Chair\", \"C\": \"table\", \"D\": \"bench\"}\n",
    "    chosenMask = np.load(f\"/gpfs/milgram/project/turk-browne/projects/rtTest/schaefer2018/{subject}/chosenMask.npy\")\n",
    "    print(f\"np.sum(chosenMask)={np.sum(chosenMask)}\")\n",
    "    # Compile preprocessed data and corresponding indices\n",
    "    metas = []\n",
    "    for run in range(1, 7):\n",
    "        print(run, end='--')\n",
    "        # retrieve from the dictionary which phase it is, assign the session\n",
    "        phase = phasedict[run]\n",
    "        \n",
    "        # Build the path for the preprocessed functional data\n",
    "        this4d = funcdata.format(run=run, phase=phase, sub=subject)\n",
    "        \n",
    "        # Read in the metadata, and reduce it to only the TR values from this run, add to a list\n",
    "        thismeta = pd.read_csv(metadata.format(run=run, phase=phase, sub=subject))\n",
    "        if dataSource == \"neurosketch\":\n",
    "            _run = 1 if run % 2 == 0 else 2\n",
    "        else:\n",
    "            _run = run\n",
    "        thismeta = thismeta[thismeta['run_num'] == int(_run)]\n",
    "        \n",
    "        if dataSource == \"realtime\":\n",
    "            TR_num = list(thismeta.TR.astype(int))\n",
    "            labels = list(thismeta.Item)\n",
    "            labels = [imcodeDict[label] for label in labels]\n",
    "        else:\n",
    "            TR_num = list(thismeta.TR_num.astype(int))\n",
    "            labels = list(thismeta.label)\n",
    "        \n",
    "        print(\"LENGTH OF TR: {}\".format(len(TR_num)))\n",
    "        # Load the functional data\n",
    "        runIm = nib.load(this4d)\n",
    "        affine_mat = runIm.affine\n",
    "        runImDat = runIm.get_fdata()\n",
    "        \n",
    "        # Use the TR numbers to select the correct features\n",
    "        features = [runImDat[:,:,:,n+3] for n in TR_num] # here shape is from (94, 94, 72, 240) to (80, 94, 94, 72)\n",
    "        features = np.array(features)\n",
    "        features = features[:, chosenMask==1]\n",
    "        print(\"shape of features\", features.shape, \"shape of chosenMask\", chosenMask.shape)\n",
    "        features = normalize(features)\n",
    "        # features = np.expand_dims(features, 0)\n",
    "        \n",
    "        # Append both so we can use it later\n",
    "        # metas.append(labels)\n",
    "        # metas['label']\n",
    "\n",
    "        t=pd.DataFrame()\n",
    "        t['label']=labels\n",
    "        t[\"run_num\"]=run\n",
    "        behav_data=t if run==1 else pd.concat([behav_data,t])\n",
    "        \n",
    "        runs = features if run == 1 else np.concatenate((runs, features))\n",
    "\n",
    "    dimsize = runIm.header.get_zooms()\n",
    "    brain_data = runs\n",
    "    print(brain_data.shape)\n",
    "    print(behav_data.shape)\n",
    "    FEAT=brain_data\n",
    "    print(f\"FEAT.shape={FEAT.shape}\")\n",
    "    META=behav_data\n",
    "\n",
    "    def Class(brain_data,behav_data):\n",
    "        accs = []\n",
    "        for run in range(1,7):\n",
    "            trainIX = behav_data['run_num']!=int(run)\n",
    "            testIX = behav_data['run_num']==int(run)\n",
    "\n",
    "            trainX =  brain_data[trainIX]\n",
    "            trainY =  behav_data.iloc[np.asarray(trainIX)].label\n",
    "\n",
    "            testX =  brain_data[testIX]\n",
    "            testY =  behav_data.iloc[np.asarray(testIX)].label\n",
    "\n",
    "            clf = LogisticRegression(penalty='l2',C=1, solver='lbfgs', max_iter=1000, \n",
    "                                    multi_class='multinomial').fit(trainX, trainY)\n",
    "\n",
    "            # Monitor progress by printing accuracy (only useful if you're running a test set)\n",
    "            acc = clf.score(testX, testY)\n",
    "            accs.append(acc)\n",
    "        accs\n",
    "        return np.mean(accs)\n",
    "    accs=Class(brain_data,behav_data)\n",
    "    print(f\"new trained 4 way classifier accuracy={accs}\")\n",
    "\n",
    "\n",
    "    # convert item colume to label colume\n",
    "    imcodeDict={\n",
    "    'A': 'bed',\n",
    "    'B': 'chair',\n",
    "    'C': 'table',\n",
    "    'D': 'bench'}\n",
    "\n",
    "    # Which run to use as test data (leave as None to not have test data)\n",
    "    testRun = 6 # when testing: testRun = 2 ; META['run_num'].iloc[:5]=2\n",
    "\n",
    "    # Decide on the proportion of crescent data to use for classification\n",
    "    include = 1\n",
    "    objects = ['bed', 'bench', 'chair', 'table']\n",
    "    allpairs = itertools.combinations(objects,2)\n",
    "    accs={}\n",
    "    # Iterate over all the possible target pairs of objects\n",
    "    for pair in allpairs:\n",
    "        # Find the control (remaining) objects for this pair\n",
    "        altpair = other(pair)\n",
    "\n",
    "        # pull sorted indices for each of the critical objects, in order of importance (low to high)\n",
    "        # inds = get_inds(FEAT, META, pair, testRun=testRun)\n",
    "\n",
    "        # Find the number of voxels that will be left given your inclusion parameter above\n",
    "        # nvox = red_vox(FEAT.shape[1], include)\n",
    "\n",
    "        for obj in pair:\n",
    "            # foil = [i for i in pair if i != obj][0]\n",
    "            for altobj in altpair:\n",
    "\n",
    "                # establish a naming convention where it is $TARGET_$CLASSIFICATION\n",
    "                # Target is the NF pair (e.g. bed/bench)\n",
    "                # Classificationis is btw one of the targets, and a control (e.g. bed/chair, or bed/table, NOT bed/bench)\n",
    "                naming = '{}{}_{}{}'.format(pair[0], pair[1], obj, altobj)\n",
    "\n",
    "                # Pull the relevant inds from your previously established dictionary \n",
    "                # obj_inds = inds[obj]\n",
    "\n",
    "                # If you're using testdata, this function will split it up. Otherwise it leaves out run as a parameter\n",
    "                # if testRun:\n",
    "                #     trainIX = META.index[(META['label'].isin([obj, altobj])) & (META['run_num'] != int(testRun))]\n",
    "                #     testIX = META.index[(META['label'].isin([obj, altobj])) & (META['run_num'] == int(testRun))]\n",
    "                # else:\n",
    "                #     trainIX = META.index[(META['label'].isin([obj, altobj]))]\n",
    "                #     testIX = META.index[(META['label'].isin([obj, altobj]))]\n",
    "                # # pull training and test data\n",
    "                # trainX = FEAT[trainIX]\n",
    "                # testX = FEAT[testIX]\n",
    "                # trainY = META.iloc[trainIX].label\n",
    "                # testY = META.iloc[testIX].label\n",
    "\n",
    "                # print(f\"obj={obj},altobj={altobj}\")\n",
    "                # print(f\"unique(trainY)={np.unique(trainY)}\")\n",
    "                # print(f\"unique(testY)={np.unique(testY)}\")\n",
    "                # assert len(np.unique(trainY))==2\n",
    "\n",
    "                # for testRun in range(6):\n",
    "                if testRun:\n",
    "                    trainIX = ((META['label']==obj) + (META['label']==altobj)) * (META['run_num']!=int(testRun))\n",
    "                    testIX = ((META['label']==obj) + (META['label']==altobj)) * (META['run_num']==int(testRun))\n",
    "                else:\n",
    "                    trainIX = ((META['label']==obj) + (META['label']==altobj))\n",
    "                    testIX = ((META['label']==obj) + (META['label']==altobj))\n",
    "                # pull training and test data\n",
    "                trainX = FEAT[trainIX]\n",
    "                testX = FEAT[testIX]\n",
    "                trainY = META.iloc[np.asarray(trainIX)].label\n",
    "                testY = META.iloc[np.asarray(testIX)].label\n",
    "\n",
    "                # print(f\"obj={obj},altobj={altobj}\")\n",
    "                # print(f\"unique(trainY)={np.unique(trainY)}\")\n",
    "                # print(f\"unique(testY)={np.unique(testY)}\")\n",
    "                assert len(np.unique(trainY))==2\n",
    "\n",
    "                # # If you're selecting high-importance features, this bit handles that\n",
    "                # if include < 1:\n",
    "                #     trainX = trainX[:, obj_inds[-nvox:]]\n",
    "                #     testX = testX[:, obj_inds[-nvox:]]\n",
    "\n",
    "                # Train your classifier\n",
    "                clf = LogisticRegression(penalty='l2',C=1, solver='lbfgs', max_iter=1000, \n",
    "                                        multi_class='multinomial').fit(trainX, trainY)\n",
    "\n",
    "\n",
    "                model_folder = f\"{working_dir}{roiloc}/{subject}/clf/\"\n",
    "                mkdir(model_folder)\n",
    "                # Save it for later use\n",
    "                joblib.dump(clf, model_folder +'/{}.joblib'.format(naming))\n",
    "\n",
    "                # Monitor progress by printing accuracy (only useful if you're running a test set)\n",
    "                acc = clf.score(testX, testY)\n",
    "                # print(naming, acc)\n",
    "                accs[naming]=acc\n",
    "    \n",
    "    # _=plt.figure()\n",
    "    # _=plt.hist(list(accs.values()))\n",
    "    return accs \n",
    "\n",
    "\n",
    "'''\n",
    "calculate the evidence floor and ceil for each subject and display different forms of evidences.\n",
    "'''\n",
    "def morphingTarget(subject,testRun=6):\n",
    "    '''\n",
    "    purpose:\n",
    "        get the morphing target function\n",
    "    steps:\n",
    "        load train clf\n",
    "        load brain data and behavior data\n",
    "        get the morphing target function\n",
    "            evidence_floor is C evidence for CD classifier(can also be D evidence for CD classifier)\n",
    "            evidence_ceil  is A evidence in AC and AD classifier\n",
    "    '''\n",
    "\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    import nibabel as nib\n",
    "\n",
    "\n",
    "    phasedict = dict(zip([1,2,3,4,5,6],[\"12\", \"12\", \"34\", \"34\", \"56\", \"56\"]))\n",
    "    imcodeDict={\"A\": \"bed\", \"B\": \"Chair\", \"C\": \"table\", \"D\": \"bench\"}\n",
    "    if 'milgram' in os.getcwd():\n",
    "        main_dir='/gpfs/milgram/project/turk-browne/projects/rtTest/'\n",
    "    else:\n",
    "        main_dir='/Users/kailong/Desktop/rtTest'\n",
    "\n",
    "    working_dir=main_dir\n",
    "    os.chdir(working_dir)\n",
    "\n",
    "    funcdata = \"/gpfs/milgram/project/turk-browne/jukebox/ntb/projects/sketchloop02/subjects/{sub}_neurosketch/data/nifti/realtime_preprocessed/{sub}_neurosketch_recognition_run_{run}.nii.gz\"\n",
    "    metadata = \"/gpfs/milgram/project/turk-browne/jukebox/ntb/projects/sketchloop02/data/features/recog/metadata_{sub}_V1_{phase}.csv\"\n",
    "\n",
    "    metas = []\n",
    "    # for run in range(1, 7):\n",
    "    #     print(run, end='--')\n",
    "    #     # retrieve from the dictionary which phase it is, assign the session\n",
    "    #     phase = phasedict[run]\n",
    "    #     ses = 1\n",
    "        \n",
    "    #     # Build the path for the preprocessed functional data\n",
    "    #     this4d = funcdata.format(ses=ses, run=run, phase=phase, sub=subject)\n",
    "        \n",
    "    #     # Read in the metadata, and reduce it to only the TR values from this run, add to a list\n",
    "    #     thismeta = pd.read_csv(metadata.format(ses=ses, run=run, phase=phase, sub=subject))\n",
    "    #     if dataSource == \"neurosketch\":\n",
    "    #         _run = 1 if run % 2 == 0 else 2\n",
    "    #     else:\n",
    "    #         _run = run\n",
    "    #     thismeta = thismeta[thismeta['run_num'] == int(_run)]\n",
    "        \n",
    "    #     if dataSource == \"realtime\":\n",
    "    #         TR_num = list(thismeta.TR.astype(int))\n",
    "    #         labels = list(thismeta.Item)\n",
    "    #         labels = [imcodeDict[label] for label in labels]\n",
    "    #     else:\n",
    "    #         TR_num = list(thismeta.TR_num.astype(int))\n",
    "    #         labels = list(thismeta.label)\n",
    "        \n",
    "    #     print(\"LENGTH OF TR: {}\".format(len(TR_num)))\n",
    "    #     # Load the functional data\n",
    "    #     runIm = nib.load(this4d)\n",
    "    #     affine_mat = runIm.affine\n",
    "    #     runImDat = runIm.get_fdata()\n",
    "        \n",
    "    #     # Use the TR numbers to select the correct features\n",
    "    #     features = [runImDat[:,:,:,n+3] for n in TR_num]\n",
    "    #     features = np.array(features)\n",
    "    #     chosenMask = np.load(f\"/gpfs/milgram/project/turk-browne/projects/rtTest/schaefer2018/{subject}/chosenMask.npy\")\n",
    "    #     features = features[:, chosenMask==1]\n",
    "    #     print(\"shape of features\", features.shape, \"shape of mask\", mask.shape)\n",
    "    #     # featmean = features.mean(1).mean(1).mean(1)[..., None,None,None] #features.mean(1)[..., None]\n",
    "    #     # features = features - featmean\n",
    "    #     # features = features - features.mean(0)\n",
    "    #     features = normalize(features)\n",
    "    #     # features = np.expand_dims(features, 0)\n",
    "        \n",
    "    #     # Append both so we can use it later\n",
    "    #     # metas.append(labels)\n",
    "    #     # metas['label']\n",
    "\n",
    "    #     t=pd.DataFrame()\n",
    "    #     t['label']=labels\n",
    "    #     t[\"run_num\"]=run\n",
    "    #     behav_data=t if run==1 else pd.concat([behav_data,t])\n",
    "        \n",
    "    #     runs = features if run == 1 else np.concatenate((runs, features))\n",
    "    # for run in range(1, 7):\n",
    "    run=testRun\n",
    "    print(run, end='--')\n",
    "    # retrieve from the dictionary which phase it is, assign the session\n",
    "    phase = phasedict[run]\n",
    "    ses = 1\n",
    "    \n",
    "    # Build the path for the preprocessed functional data\n",
    "    this4d = funcdata.format(ses=ses, run=run, phase=phase, sub=subject)\n",
    "    \n",
    "    # Read in the metadata, and reduce it to only the TR values from this run, add to a list\n",
    "    thismeta = pd.read_csv(metadata.format(ses=ses, run=run, phase=phase, sub=subject))\n",
    "    if dataSource == \"neurosketch\":\n",
    "        _run = 1 if run % 2 == 0 else 2\n",
    "    else:\n",
    "        _run = run\n",
    "    thismeta = thismeta[thismeta['run_num'] == int(_run)]\n",
    "    \n",
    "    if dataSource == \"realtime\":\n",
    "        TR_num = list(thismeta.TR.astype(int))\n",
    "        labels = list(thismeta.Item)\n",
    "        labels = [imcodeDict[label] for label in labels]\n",
    "    else:\n",
    "        TR_num = list(thismeta.TR_num.astype(int))\n",
    "        labels = list(thismeta.label)\n",
    "    \n",
    "    print(\"LENGTH OF TR: {}\".format(len(TR_num)))\n",
    "    # Load the functional data\n",
    "    runIm = nib.load(this4d)\n",
    "    affine_mat = runIm.affine\n",
    "    runImDat = runIm.get_fdata()\n",
    "    \n",
    "    # Use the TR numbers to select the correct features\n",
    "    features = [runImDat[:,:,:,n+3] for n in TR_num]\n",
    "    features = np.array(features)\n",
    "    chosenMask = np.load(f\"/gpfs/milgram/project/turk-browne/projects/rtTest/schaefer2018/{subject}/chosenMask.npy\")\n",
    "    features = features[:, chosenMask==1]\n",
    "    print(\"shape of features\", features.shape, \"shape of mask\", mask.shape)\n",
    "    # featmean = features.mean(1).mean(1).mean(1)[..., None,None,None] #features.mean(1)[..., None]\n",
    "    # features = features - featmean\n",
    "    # features = features - features.mean(0)\n",
    "    features = normalize(features)\n",
    "    # features = np.expand_dims(features, 0)\n",
    "    \n",
    "    # Append both so we can use it later\n",
    "    # metas.append(labels)\n",
    "    # metas['label']\n",
    "\n",
    "    t=pd.DataFrame()\n",
    "    t['label']=labels\n",
    "    t[\"run_num\"]=run\n",
    "    behav_data=t\n",
    "    \n",
    "    runs = features\n",
    "\n",
    "    \n",
    "    dimsize = runIm.header.get_zooms()\n",
    "    \n",
    "    brain_data = runs\n",
    "    print(brain_data.shape)\n",
    "    print(behav_data.shape)\n",
    "    FEAT=brain_data\n",
    "    print(f\"FEAT.shape={FEAT.shape}\")\n",
    "    META=behav_data\n",
    "\n",
    "    # print('mask dimensions: {}'. format(mask.shape))\n",
    "    # print('number of voxels in mask: {}'.format(np.sum(mask)))\n",
    "\n",
    "    # runRecording = pd.read_csv(f\"{cfg.recognition_dir}../runRecording.csv\")\n",
    "    # actualRuns = list(runRecording['run'].iloc[list(np.where(1==1*(runRecording['type']=='recognition'))[0])]) # can be [1,2,3,4,5,6,7,8] or [1,2,4,5]\n",
    "\n",
    "    # objects = ['bed', 'bench', 'chair', 'table']\n",
    "\n",
    "    # for ii,run in enumerate(actualRuns[:2]): # load behavior and brain data for current session\n",
    "    #     t = np.load(f\"{cfg.recognition_dir}brain_run{run}.npy\")\n",
    "    #     # mask = nib.load(f\"{cfg.chosenMask}\").get_data()\n",
    "    #     mask = np.load(cfg.chosenMask)\n",
    "    #     t = t[:,mask==1]\n",
    "    #     t = normalize(t)\n",
    "    #     brain_data=t if ii==0 else np.concatenate((brain_data,t), axis=0)\n",
    "\n",
    "    #     t = pd.read_csv(f\"{cfg.recognition_dir}behav_run{run}.csv\")\n",
    "    #     behav_data=t if ii==0 else pd.concat([behav_data,t])\n",
    "\n",
    "    # FEAT=brain_data.reshape(brain_data.shape[0],-1)\n",
    "    # # FEAT_mean=np.mean(FEAT,axis=1)\n",
    "    # # FEAT=(FEAT.T-FEAT_mean).T\n",
    "    # # FEAT_mean=np.mean(FEAT,axis=0)\n",
    "    # # FEAT=FEAT-FEAT_mean\n",
    "\n",
    "    # META=behav_data\n",
    "\n",
    "    # convert item colume to label colume\n",
    "    imcodeDict={\n",
    "    'A': 'bed',\n",
    "    'B': 'chair',\n",
    "    'C': 'table',\n",
    "    'D': 'bench'}\n",
    "    # label=[]\n",
    "    # for curr_trial in range(META.shape[0]):\n",
    "    #     label.append(imcodeDict[META['Item'].iloc[curr_trial]])\n",
    "    # META['label']=label # merge the label column with the data dataframe\n",
    "\n",
    "\n",
    "    # def classifierEvidence(clf,X,Y): # X shape is [trials,voxelNumber], Y is ['bed', 'bed'] for example # return a 1-d array of probability\n",
    "    #     # This function get the data X and evidence object I want to know Y, and output the trained model evidence.\n",
    "    #     targetID=[np.where((clf.classes_==i)==True)[0][0] for i in Y]\n",
    "    #     # Evidence=(np.sum(X*clf.coef_,axis=1)+clf.intercept_) if targetID[0]==1 else (1-(np.sum(X*clf.coef_,axis=1)+clf.intercept_))\n",
    "    #     Evidence=(X@clf.coef_.T+clf.intercept_) if targetID[0]==1 else (-(X@clf.coef_.T+clf.intercept_))\n",
    "    #     Evidence = 1/(1+np.exp(-Evidence))\n",
    "    #     return np.asarray(Evidence)\n",
    "\n",
    "    # def classifierEvidence(clf,X,Y):\n",
    "    #     ID=np.where((clf.classes_==Y[0])*1==1)[0][0]\n",
    "    #     p = clf.predict_proba(X)[:,ID]\n",
    "    #     BX=np.log(p/(1-p))\n",
    "    #     return BX\n",
    "\n",
    "    def classifierEvidence(clf,X,Y):\n",
    "        ID=np.where((clf.classes_==Y[0])*1==1)[0][0]\n",
    "        Evidence=(X@clf.coef_.T+clf.intercept_) if ID==1 else (-(X@clf.coef_.T+clf.intercept_))\n",
    "        # Evidence=(X@clf.coef_.T+clf.intercept_) if ID==0 else (-(X@clf.coef_.T+clf.intercept_))\n",
    "        return np.asarray(Evidence)\n",
    "\n",
    "    A_ID = (META['label']=='bed')\n",
    "    X = FEAT[A_ID]\n",
    "\n",
    "    # evidence_floor is C evidence for AC_CD BC_CD CD_CD classifier(can also be D evidence for CD classifier)\n",
    "    # Y = ['table'] * X.shape[0]\n",
    "    # CD_clf=joblib.load(cfg.usingModel_dir +'bedbench_benchtable.joblib') # These 4 clf are the same: bedbench_benchtable.joblib bedtable_tablebench.joblib benchchair_benchtable.joblib chairtable_tablebench.joblib\n",
    "    # CD_C_evidence = classifierEvidence(CD_clf,X,Y)\n",
    "    # evidence_floor = np.mean(CD_C_evidence)\n",
    "    # print(f\"evidence_floor={evidence_floor}\")\n",
    "\n",
    "    model_folder = f\"{working_dir}{roiloc}/{subject}/clf/\"\n",
    "\n",
    "    # #try out other forms of floor: C evidence in AC and D evidence for AD\n",
    "    # Y = ['bench'] * X.shape[0]\n",
    "    # AD_clf=joblib.load(model_folder +'bedchair_bedbench.joblib') # These 4 clf are the same:   bedchair_bedbench.joblib bedtable_bedbench.joblib benchchair_benchbed.joblib benchtable_benchbed.joblib\n",
    "    # AD_D_evidence = classifierEvidence(AD_clf,X,Y)\n",
    "    # evidence_floor = np.mean(AD_D_evidence)\n",
    "    # print(f\"evidence_floor2={np.mean(evidence_floor)}\")\n",
    "\n",
    "\n",
    "\n",
    "    # # floor\n",
    "    # Y = ['bench'] * X.shape[0]\n",
    "    # CD_clf=joblib.load(model_folder +'bedbench_benchtable.joblib') # These 4 clf are the same: bedbench_benchtable.joblib bedtable_tablebench.joblib benchchair_benchtable.joblib chairtable_tablebench.joblib\n",
    "    # CD_D_evidence = classifierEvidence(CD_clf,X,Y)\n",
    "    # evidence_floor = np.mean(CD_D_evidence)\n",
    "    # print(f\"evidence_floor={evidence_floor}\")\n",
    "\n",
    "    # Y = ['table'] * X.shape[0]\n",
    "    # CD_clf=joblib.load(model_folder +'bedbench_benchtable.joblib') # These 4 clf are the same: bedbench_benchtable.joblib bedtable_tablebench.joblib benchchair_benchtable.joblib chairtable_tablebench.joblib\n",
    "    # CD_C_evidence = classifierEvidence(CD_clf,X,Y)\n",
    "    # evidence_floor = np.mean(CD_C_evidence)\n",
    "    # print(f\"evidence_floor={evidence_floor}\")\n",
    "\n",
    "\n",
    "    # # evidence_ceil  is A evidence in AC and AD classifier\n",
    "    # Y = ['bed'] * X.shape[0]\n",
    "    # AC_clf=joblib.load(model_folder +'benchtable_tablebed.joblib') # These 4 clf are the same:   bedbench_bedtable.joblib bedchair_bedtable.joblib benchtable_tablebed.joblib chairtable_tablebed.joblib\n",
    "    # AC_A_evidence = classifierEvidence(AC_clf,X,Y)\n",
    "    # evidence_ceil1 = AC_A_evidence\n",
    "    # print(f\"evidence_ceil1={np.mean(evidence_ceil1)}\")\n",
    "\n",
    "    # Y = ['bed'] * X.shape[0]\n",
    "    # AD_clf=joblib.load(model_folder +'bedchair_bedbench.joblib') # These 4 clf are the same:   bedchair_bedbench.joblib bedtable_bedbench.joblib benchchair_benchbed.joblib benchtable_benchbed.joblib\n",
    "    # AD_A_evidence = classifierEvidence(AD_clf,X,Y)\n",
    "    # evidence_ceil2 = AD_A_evidence\n",
    "    # print(f\"evidence_ceil2={np.mean(evidence_ceil2)}\")\n",
    "\n",
    "    # # evidence_ceil = np.mean(evidence_ceil1)\n",
    "    # # evidence_ceil = np.mean(evidence_ceil2)\n",
    "    # evidence_ceil = np.mean((evidence_ceil1+evidence_ceil2)/2)\n",
    "    # print(f\"evidence_ceil={evidence_ceil}\")\n",
    "    store=\"\\n\"\n",
    "    print(\"floor\")\n",
    "    # D evidence for AD_clf when A is presented.\n",
    "    Y = ['bench'] * X.shape[0]\n",
    "    AD_clf=joblib.load(model_folder +'bedchair_bedbench.joblib') # These 4 clf are the same:   bedchair_bedbench.joblib bedtable_bedbench.joblib benchchair_benchbed.joblib benchtable_benchbed.joblib\n",
    "    AD_D_evidence = classifierEvidence(AD_clf,X,Y)\n",
    "    evidence_floor = np.mean(AD_D_evidence)\n",
    "    print(f\"D evidence for AD_clf when A is presented={evidence_floor}\")\n",
    "    store=store+f\"D evidence for AD_clf when A is presented={evidence_floor}\"\n",
    "\n",
    "    # C evidence for AC_clf when A is presented.\n",
    "    Y = ['table'] * X.shape[0]\n",
    "    AC_clf=joblib.load(model_folder +'benchtable_tablebed.joblib') # These 4 clf are the same:   bedbench_bedtable.joblib bedchair_bedtable.joblib benchtable_tablebed.joblib chairtable_tablebed.joblib\n",
    "    AC_C_evidence = classifierEvidence(AC_clf,X,Y)\n",
    "    evidence_floor = np.mean(AC_C_evidence)\n",
    "    print(f\"C evidence for AC_clf when A is presented={evidence_floor}\")\n",
    "    store=store+\"\\n\"+f\"C evidence for AC_clf when A is presented={evidence_floor}\"\n",
    "\n",
    "    # D evidence for CD_clf when A is presented.\n",
    "    Y = ['bench'] * X.shape[0]\n",
    "    CD_clf=joblib.load(model_folder +'bedbench_benchtable.joblib') # These 4 clf are the same: bedbench_benchtable.joblib bedtable_tablebench.joblib benchchair_benchtable.joblib chairtable_tablebench.joblib\n",
    "    CD_D_evidence = classifierEvidence(CD_clf,X,Y)\n",
    "    evidence_floor = np.mean(CD_D_evidence)\n",
    "    print(f\"D evidence for CD_clf when A is presented={evidence_floor}\")\n",
    "    store=store+\"\\n\"+f\"D evidence for CD_clf when A is presented={evidence_floor}\"\n",
    "\n",
    "    # C evidence for CD_clf when A is presented.\n",
    "    Y = ['table'] * X.shape[0]\n",
    "    CD_clf=joblib.load(model_folder +'bedbench_benchtable.joblib') # These 4 clf are the same: bedbench_benchtable.joblib bedtable_tablebench.joblib benchchair_benchtable.joblib chairtable_tablebench.joblib\n",
    "    CD_C_evidence = classifierEvidence(CD_clf,X,Y)\n",
    "    evidence_floor = np.mean(CD_C_evidence)\n",
    "    print(f\"C evidence for CD_clf when A is presented={evidence_floor}\")\n",
    "    store=store+\"\\n\"+f\"C evidence for CD_clf when A is presented={evidence_floor}\"\n",
    "\n",
    "\n",
    "    print(\"ceil\")\n",
    "    store=store+\"\\n\"+\"ceil\"\n",
    "    # evidence_ceil  is A evidence in AC and AD classifier\n",
    "    Y = ['bed'] * X.shape[0]\n",
    "    AC_clf=joblib.load(model_folder +'benchtable_tablebed.joblib') # These 4 clf are the same:   bedbench_bedtable.joblib bedchair_bedtable.joblib benchtable_tablebed.joblib chairtable_tablebed.joblib\n",
    "    AC_A_evidence = classifierEvidence(AC_clf,X,Y)\n",
    "    evidence_ceil1 = AC_A_evidence\n",
    "    print(f\"A evidence in AC_clf when A is presented={np.mean(evidence_ceil1)}\")\n",
    "    store=store+\"\\n\"+f\"A evidence in AC_clf when A is presented={np.mean(evidence_ceil1)}\"\n",
    "\n",
    "    Y = ['bed'] * X.shape[0]\n",
    "    AD_clf=joblib.load(model_folder +'bedchair_bedbench.joblib') # These 4 clf are the same:   bedchair_bedbench.joblib bedtable_bedbench.joblib benchchair_benchbed.joblib benchtable_benchbed.joblib\n",
    "    AD_A_evidence = classifierEvidence(AD_clf,X,Y)\n",
    "    evidence_ceil2 = AD_A_evidence\n",
    "    print(f\"A evidence in AD_clf when A is presented={np.mean(evidence_ceil2)}\")\n",
    "    store=store+\"\\n\"+f\"A evidence in AD_clf when A is presented={np.mean(evidence_ceil2)}\"\n",
    "\n",
    "    # evidence_ceil = np.mean(evidence_ceil1)\n",
    "    # evidence_ceil = np.mean(evidence_ceil2)\n",
    "    evidence_ceil = np.mean((evidence_ceil1+evidence_ceil2)/2)\n",
    "    print(f\"evidence_ceil={evidence_ceil}\")\n",
    "    store=store+\"\\n\"+f\"evidence_ceil={evidence_ceil}\"\n",
    "    ceil,floor=evidence_ceil,evidence_floor\n",
    "    mu = (ceil+floor)/2\n",
    "    sig = (ceil-floor)/2.3548\n",
    "    print(f\"floor={floor}, ceil={ceil}\")\n",
    "    print(f\"mu={mu}, sig={sig}\")\n",
    "\n",
    "    store=store+\"\\n\"+f\"floor={floor}, ceil={ceil}\"\n",
    "    store=store+\"\\n\"+f\"mu={mu}, sig={sig}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    BC_clf=joblib.load(model_folder +'benchchair_chairtable.joblib') # These 4 clf are the same: bedbench_benchtable.joblib bedtable_tablebench.joblib benchchair_benchtable.joblib chairtable_tablebench.joblib\n",
    "    BD_clf=joblib.load(model_folder +'bedchair_chairbench.joblib') # These 4 clf are the same: bedbench_benchtable.joblib bedtable_tablebench.joblib benchchair_benchtable.joblib chairtable_tablebench.joblib\n",
    "    Y = ['chair']*FEAT.shape[0]\n",
    "    # imcodeDict={\n",
    "    # 'A': 'bed',\n",
    "    # 'B': 'chair',\n",
    "    # 'C': 'table',\n",
    "    # 'D': 'bench'}\n",
    "    print(f\"classifierEvidence(BC_clf,FEAT,Y)={classifierEvidence(BC_clf,FEAT,Y)}\")\n",
    "    print(f\"classifierEvidence(BD_clf,FEAT,Y)={classifierEvidence(BD_clf,FEAT,Y)}\")\n",
    "    BC_B_evidence = classifierEvidence(BC_clf,X,Y)\n",
    "    BD_B_evidence = classifierEvidence(BD_clf,X,Y)\n",
    "    print(f\"BC_B_evidence={BC_B_evidence}\")\n",
    "    print(f\"BD_B_evidence={BD_B_evidence}\")\n",
    "    B_evidence = (BC_B_evidence+BD_B_evidence)/2\n",
    "    print(f\"B_evidence={B_evidence}\")\n",
    "    store=store+\"\\n\"+B_evidence\n",
    "    print(f\"mu={mu}, sig={sig}\")\n",
    "    def gaussian(x, mu, sig):\n",
    "        # mu and sig is determined before each neurofeedback session using 2 recognition runs.\n",
    "        return round(1+18*(1 - np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.))))) # map from (0,1) -> [1,19]\n",
    "    morphParam=int(gaussian(B_evidence, mu, sig))\n",
    "    # B_evidences.append(B_evidence)\n",
    "    print(f\"morphParam={morphParam}\")\n",
    "\n",
    "    return evidence_floor, evidence_ceil,store\n",
    "    \n",
    "    \n",
    "# sub_id=7\n",
    "import sys\n",
    "\n",
    "# subject= '0119173' #sys.argv[1]\n",
    "# sub_id = [i for i,x in enumerate(subjects) if x == subject][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def subLoop(subject):\n",
    "    data={}\n",
    "    accs = minimalClass(subject)\n",
    "    print(\"best 4way classifier accuracy = \",GreedyBestAcc[subject][bestID[subject]])\n",
    "    data['best 4way classifier accuracy']=GreedyBestAcc[subject][bestID[subject]]\n",
    "    for acc in accs:\n",
    "        print(acc,accs[acc])\n",
    "    data[\"accs\"]=accs\n",
    "    floor, ceil,store = morphingTarget(subject,testRun=6)\n",
    "    data[\"store testing run\"]=store\n",
    "    floor, ceil,store = morphingTarget(subject,testRun=1)\n",
    "    data[\"store training run\"]=store\n",
    "    \n",
    "    save_obj(store,f\"./{subject}store\")\n",
    "    return data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "data={}\n",
    "for subject in tqdm(subjects):\n",
    "    data[subject]=subLoop(subject)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # floorCeilNeurosketch_child.sh\n",
    "# #!/usr/bin/env bash\n",
    "# # Input python command to be submitted as a job\n",
    "# #SBATCH --output=logs/floorCeil-%j.out\n",
    "# #SBATCH --job-name floorCeil\n",
    "# #SBATCH --partition=short,day,scavenge,verylong\n",
    "# #SBATCH --time=1:00:00 #20:00:00\n",
    "# #SBATCH --mem=10000\n",
    "# #SBATCH -n 5\n",
    "\n",
    "# # Set up the environment\n",
    "\n",
    "# subject=$1\n",
    "\n",
    "# echo source activate /gpfs/milgram/project/turk-browne/users/kp578/CONDA/rtcloud\n",
    "# source activate /gpfs/milgram/project/turk-browne/users/kp578/CONDA/rtcloud\n",
    "\n",
    "# python -u ./floorCeilNeurosketch.py $subject\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # floorCeilNeurosketch_parent.sh\n",
    "# subjects=\"1206161 0119173 1206162 1130161 1206163 0120171 0111171 1202161 0125172 0110172 0123173 0120173 0110171 0119172 0124171 0123171 1203161 0118172 0118171 0112171 1207162 0117171 0119174 0112173 0112172\" #these subjects are done with the batchRegions code\n",
    "# for sub in $subjects\n",
    "# do\n",
    "#   for num in 25; #best ID is 30 thus the best num is 31\n",
    "#   do\n",
    "#     echo sbatch --requeue floorCeilNeurosketch_child.sh $sub\n",
    "#     sbatch --requeue floorCeilNeurosketch_child.sh $sub\n",
    "#   done\n",
    "# done\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
