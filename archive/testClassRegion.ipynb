{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  this script is meant to deal with the data of 8 recognition runs and generate models saved in corresponding folder\n",
    "'''\n",
    "input:\n",
    "    cfg.session=ses1\n",
    "    cfg.modelFolder=f\"{cfg.subjects_dir}/{cfg.subjectName}/{cfg.session}_recognition/clf/\"\n",
    "    cfg.dataFolder=f\"{cfg.subjects_dir}/{cfg.subjectName}/{cfg.session}_recognition/\"\n",
    "output:\n",
    "    models in cfg.modelFolder\n",
    "'''\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/gpfs/milgram/project/turk-browne/projects/rtSynth_rt/')\n",
    "import argparse\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import scipy.io as sio\n",
    "from subprocess import call\n",
    "from nibabel.nicom import dicomreaders\n",
    "import pydicom as dicom  # type: ignore\n",
    "import time\n",
    "from glob import glob\n",
    "import shutil\n",
    "from nilearn.image import new_img_like\n",
    "import joblib\n",
    "import rtCommon.utils as utils\n",
    "from rtCommon.utils import loadConfigFile\n",
    "from rtCommon.fileClient import FileInterface\n",
    "import rtCommon.projectUtils as projUtils\n",
    "from rtCommon.imageHandling import readRetryDicomFromFileInterface, getDicomFileName, convertDicomImgToNifti\n",
    "\n",
    "\n",
    "# argParser = argparse.ArgumentParser()\n",
    "# argParser.add_argument('--config', '-c', default='sub001.ses1.toml', type=str, help='experiment file (.json or .toml)')\n",
    "# args = argParser.parse_args()\n",
    "from rtCommon.cfg_loading import mkdir,cfg_loading\n",
    "cfg = cfg_loading(\"sub001.ses1.toml\")\n",
    "\n",
    "sys.path.append('/gpfs/milgram/project/turk-browne/projects/rtSynth_rt/expScripts/recognition/')\n",
    "from recognition_dataAnalysisFunctions import recognition_preprocess,minimalClass,behaviorDataLoading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO ROI LOCATION ENTERED: Using radius of wang2014\n",
      "NO DATASOURCE ENTERED: Using original neurosketch data\n",
      "NO ROI SPECIFIED: Using roi number 1\n",
      "Since this is wang2014, we need a hemisphere, in this case _lh\n",
      "Running subject sub001, with neurosketch as a data source, wang2014 roi #1 _lh\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This script is adapted from classRegion.py\n",
    "Purpose:\n",
    "    to train and save the classifiers for all ROIs\n",
    "\n",
    "'''\n",
    " \n",
    "\n",
    "'''\n",
    "from the recognition exp dir, run batchRegions.sh, it will run the script classRegion.sh, which is just a feeder for classRegion.py for all ROI/parcels across both wang and schaefer.\n",
    "\n",
    "classRegion.py simply runs a runwise cross-validated classifier across the runs of recognition data, then stores the average accuracy of the ROI it was assigned in an numpy array. \n",
    "This is stored within the subject specific folder (e.g. wang2014/0111171/output/roi25_rh.npy )\n",
    "\n",
    "input:\n",
    "    1 subject: which subject\n",
    "    2 dataloc: neurosketch or realtime\n",
    "    3 roiloc: schaefer2018 or wang2014\n",
    "    4 roinum: number of rois you want\n",
    "    5 roihemi: which hemisphere\n",
    "\n",
    "'''\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# What subject are you running\n",
    "subject = \"sub001\" #sys.argv[1]\n",
    "dataSource = \"realtime\"\n",
    "recognition_dir = '/gpfs/milgram/project/turk-browne/projects/rtSynth_rt/subjects/sub001/ses1/recognition/' #sys.argv[1]\n",
    "\n",
    "print(\"NO ROI LOCATION ENTERED: Using radius of wang2014\")\n",
    "roiloc = \"wang\"\n",
    "\n",
    "print(\"NO DATASOURCE ENTERED: Using original neurosketch data\")\n",
    "dataSource = 'neurosketch'\n",
    "\n",
    "print(\"NO ROI SPECIFIED: Using roi number 1\")\n",
    "roinum=\"1\"\n",
    "\n",
    "if roiloc == \"wang2014\":\n",
    "    try:\n",
    "        roihemi = \"_{}\".format(\"lh\")\n",
    "        print(\"Since this is wang2014, we need a hemisphere, in this case {}\".format(roihemi))\n",
    "    except:\n",
    "        print(\"this is wang 2014, so we need a hemisphere, but one was not specified\")\n",
    "        assert 1 == 2\n",
    "else:\n",
    "    roihemi=\"\"\n",
    "\n",
    "print(\"Running subject {}, with {} as a data source, {} roi #{} {}\".format(subject, dataSource, roiloc, roinum, roihemi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask dimensions: (64, 64, 36)\n",
      "number of voxels in mask: 271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/milgram/project/turk-browne/users/kp578/CONDA/rtcloud/lib/python3.6/site-packages/ipykernel_launcher.py:61: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# dataSource depending, there are a number of keywords to fill in: \n",
    "# ses: which day of data collection\n",
    "# run: which run number on that day (single digit)\n",
    "# phase: 12, 34, or 56\n",
    "# sub: subject number\n",
    "if dataSource == \"neurosketch\":\n",
    "    funcdata = \"/gpfs/milgram/project/turk-browne/jukebox/ntb/projects/sketchloop02/subjects/{sub}_neurosketch/data/nifti/realtime_preprocessed/{sub}_neurosketch_recognition_run_{run}.nii.gz\"\n",
    "    metadata = \"/gpfs/milgram/project/turk-browne/jukebox/ntb/projects/sketchloop02/data/features/recog/metadata_{sub}_V1_{phase}.csv\"\n",
    "    anat = \"/gpfs/milgram/project/turk-browne/jukebox/ntb/projects/sketchloop02/subjects/{sub}_neurosketch/data/nifti/{sub}_neurosketch_anat_mprage_brain.nii.gz\"\n",
    "elif dataSource == \"realtime\":\n",
    "    funcdata = \"{recognition_dir}run{run}.nii.gz\"\n",
    "    metadata = \"{recognition_dir}{subject}_{run_i}.csv\"\n",
    "    anat = \"$TO_BE_FILLED\"\n",
    "else:\n",
    "    funcdata = \"/gpfs/milgram/project/turk-browne/projects/rtTest/searchout/feat/{sub}_pre.nii.gz\"\n",
    "    metadata = \"/gpfs/milgram/project/turk-browne/jukebox/ntb/projects/sketchloop02/data/features/recog/metadata_{sub}_V1_{phase}.csv\"\n",
    "    anat = \"$TO_BE_FILLED\"\n",
    "    \n",
    "outloc = \"/gpfs/milgram/project/turk-browne/projects/rtTest/searchout\"\n",
    "starttime = time.time()\n",
    "\n",
    "\n",
    "def Wait(waitfor, delay=1):\n",
    "    while not os.path.exists(waitfor):\n",
    "        time.sleep(delay)\n",
    "        print('waiting for {}'.format(waitfor))\n",
    "        \n",
    "def normalize(X):\n",
    "    X = X - X.mean(3)\n",
    "    return X\n",
    "\n",
    "def Class(data, bcvar):\n",
    "    metas = bcvar[0]\n",
    "    data4d = data[0]\n",
    "    print(data4d.shape)\n",
    "\n",
    "    accs = []\n",
    "    for run in range(6):\n",
    "        testX = data4d[run]\n",
    "        testY = metas[run]\n",
    "        trainX = data4d[np.arange(6) != run]\n",
    "        trainX = trainX.reshape(trainX.shape[0]*trainX.shape[1], -1)\n",
    "        trainY = []\n",
    "        for meta in range(6):\n",
    "            if meta != run:\n",
    "                trainY.extend(metas[run])\n",
    "        clf = LogisticRegression(penalty='l2',C=1, solver='lbfgs', max_iter=1000, \n",
    "                                 multi_class='multinomial').fit(trainX, trainY)\n",
    "                \n",
    "        # Monitor progress by printing accuracy (only useful if you're running a test set)\n",
    "        acc = clf.score(testX, testY)\n",
    "        accs.append(acc)\n",
    "    \n",
    "    return np.mean(accs)\n",
    "\n",
    "\n",
    "# phasedict = dict(zip([1,2,3,4,5,6,7,8],[\"12\", \"12\", \"34\", \"34\", \"56\", \"56\"]))\n",
    "phasedict = dict(zip([1,2,3,4,5,6,7,8],[cfg.actualRuns]))\n",
    "imcodeDict={\"A\": \"bed\", \"B\": \"Chair\", \"C\": \"table\", \"D\": \"bench\"}\n",
    "\n",
    "mask = nib.load(f\"{cfg.mask_dir}{roiloc}_roi{roinum}{roihemi}.nii.gz\").get_data()\n",
    "mask = mask.astype(int)# say some things about the mask.\n",
    "print('mask dimensions: {}'. format(mask.shape))\n",
    "print('number of voxels in mask: {}'.format(np.sum(mask)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2--"
     ]
    }
   ],
   "source": [
    "run_i,run=0,cfg.actualRuns[0]\n",
    "print(run, end='--')\n",
    "# retrieve from the dictionary which phase it is, assign the session\n",
    "# Build the path for the preprocessed functional data\n",
    "this4d = f\"{cfg.recognition_dir}run{run}.nii.gz\" # run data\n",
    "\n",
    "# Read in the metadata, and reduce it to only the TR values from this run, add to a list\n",
    "thismeta = pd.read_csv(f\"{cfg.recognition_dir}{cfg.subjectName}_{run_i+1}.csv\")\n",
    "# thismeta = thismeta[thismeta['run_num'] == int(run)]\n",
    "TR_num = list(thismeta.TR.astype(int))\n",
    "labels = list(thismeta.Item)\n",
    "labels = [None if type(label)==float else imcodeDict[label] for label in labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LENGTH OF TR: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/milgram/project/turk-browne/users/kp578/CONDA/rtcloud/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "print(\"LENGTH OF TR: {}\".format(len(TR_num)))\n",
    "# Load the functional data\n",
    "runIm = nib.load(this4d)\n",
    "affine_mat = runIm.affine\n",
    "runImDat = runIm.get_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of features (140, 271) shape of mask (64, 64, 36)\n"
     ]
    }
   ],
   "source": [
    "# Use the TR numbers to select the correct features\n",
    "features = [runImDat[:,:,:,n+2] for n in TR_num]\n",
    "features = np.array(features)\n",
    "features = features[:, mask==1]\n",
    "print(\"shape of features\", features.shape, \"shape of mask\", mask.shape)\n",
    "featmean = features.mean(1)[..., None]\n",
    "features = features - featmean\n",
    "features = np.expand_dims(features, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2--LENGTH OF TR: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/milgram/project/turk-browne/users/kp578/CONDA/rtcloud/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of features (140, 271) shape of mask (64, 64, 36)\n",
      "3--LENGTH OF TR: 148\n",
      "shape of features (148, 271) shape of mask (64, 64, 36)\n",
      "4--LENGTH OF TR: 147\n",
      "shape of features (147, 271) shape of mask (64, 64, 36)\n",
      "5--LENGTH OF TR: 143\n",
      "shape of features (143, 271) shape of mask (64, 64, 36)\n",
      "6--LENGTH OF TR: 146\n",
      "shape of features (146, 271) shape of mask (64, 64, 36)\n",
      "7--LENGTH OF TR: 148\n",
      "shape of features (148, 271) shape of mask (64, 64, 36)\n",
      "9--LENGTH OF TR: 139\n",
      "shape of features (139, 271) shape of mask (64, 64, 36)\n",
      "10--LENGTH OF TR: 145\n",
      "shape of features (145, 271) shape of mask (64, 64, 36)\n"
     ]
    }
   ],
   "source": [
    "# Compile preprocessed data and corresponding indices\n",
    "metas = []\n",
    "runs=[]\n",
    "for run_i,run in enumerate(cfg.actualRuns):\n",
    "    print(run, end='--')\n",
    "    # Build the path for the preprocessed functional data\n",
    "    this4d = f\"{cfg.recognition_dir}run{run}.nii.gz\" # run data\n",
    "    \n",
    "    # Read in the metadata, and reduce it to only the TR values from this run, add to a list\n",
    "    thismeta = pd.read_csv(f\"{cfg.recognition_dir}{cfg.subjectName}_{run_i+1}.csv\")\n",
    "\n",
    "    TR_num = list(thismeta.TR.astype(int))\n",
    "    labels = list(thismeta.Item)\n",
    "    labels = [None if type(label)==float else imcodeDict[label] for label in labels]\n",
    "\n",
    "\n",
    "    print(\"LENGTH OF TR: {}\".format(len(TR_num)))\n",
    "    # Load the functional data\n",
    "    runIm = nib.load(this4d)\n",
    "    affine_mat = runIm.affine\n",
    "    runImDat = runIm.get_data()\n",
    "    \n",
    "    # Use the TR numbers to select the correct features\n",
    "    features = [runImDat[:,:,:,n+2] for n in TR_num]\n",
    "    features = np.array(features)\n",
    "    features = features[:, mask==1]\n",
    "    print(\"shape of features\", features.shape, \"shape of mask\", mask.shape)\n",
    "    featmean = features.mean(1)[..., None]\n",
    "    features = features - featmean\n",
    "    \n",
    "    # Append both so we can use it later\n",
    "    metas.append(labels)\n",
    "    runs.append(features) # if run_i == 0 else np.concatenate((runs, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1016, 271)\n",
      "980\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(len(trainY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/milgram/project/turk-browne/users/kp578/CONDA/rtcloud/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/gpfs/milgram/project/turk-browne/users/kp578/CONDA/rtcloud/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/gpfs/milgram/project/turk-browne/users/kp578/CONDA/rtcloud/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/gpfs/milgram/project/turk-browne/users/kp578/CONDA/rtcloud/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/gpfs/milgram/project/turk-browne/users/kp578/CONDA/rtcloud/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/gpfs/milgram/project/turk-browne/users/kp578/CONDA/rtcloud/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/gpfs/milgram/project/turk-browne/users/kp578/CONDA/rtcloud/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/gpfs/milgram/project/turk-browne/users/kp578/CONDA/rtcloud/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def Class(data, bcvar):\n",
    "    metas = bcvar\n",
    "    data4d = data\n",
    "    accs = []\n",
    "    for curr_run in range(8):\n",
    "        testX = data4d[curr_run]\n",
    "        testY = metas[curr_run]\n",
    "        trainX=None\n",
    "        for train_run in range(8):\n",
    "            if train_run!=curr_run:\n",
    "                trainX = data4d[train_run] if type(trainX)!=np.ndarray else np.concatenate((trainX, data4d[train_run]),axis=0)\n",
    "        trainY = []\n",
    "        for train_run in range(8):\n",
    "            if train_run!=curr_run:\n",
    "                trainY.extend(metas[train_run])\n",
    "        # remove nan type\n",
    "        id=[type(i)==str for i in trainY]\n",
    "        trainY=[i for i in trainY if type(i)==str]\n",
    "        trainX=trainX[id]\n",
    "\n",
    "        clf = LogisticRegression(penalty='l2',C=1, solver='lbfgs', max_iter=1000, \n",
    "                                 multi_class='multinomial').fit(trainX, trainY)\n",
    "\n",
    "        # Monitor progress by printing accuracy (only useful if you're running a test set)\n",
    "        id=[type(i)==str for i in testY]\n",
    "        testY=[i for i in testY if type(i)==str]\n",
    "        testX=testX[id]\n",
    "        acc = clf.score(testX, testY)\n",
    "        accs.append(acc)\n",
    "    return np.mean(accs)\n",
    "accs=Class(data, bcvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5833333333333334,\n",
       " 0.6875,\n",
       " 0.4791666666666667,\n",
       " 0.5833333333333334,\n",
       " 0.5833333333333334,\n",
       " 0.5833333333333334,\n",
       " 0.5416666666666666,\n",
       " 0.3958333333333333]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bash /gpfs/milgram/project/turk-browne/projects/rtSynth_rt/expScripts/recognition/batchRegions.sh sub001.ses1.toml'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "command=f\"bash {cfg.recognition_expScripts_dir}batchRegions.sh sub001.ses1.toml\"\n",
    "command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/gpfs/milgram/project/turk-browne/projects/rtSynth_rt/expScripts/recognition/'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.recognition_expScripts_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/gpfs/milgram/project/turk-browne/projects/rtSynth_rt/subjects/sub001/ses1/recognition/mask/wang_1_lh.nii.gz'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{cfg.mask_dir}{roiloc}_{roinum}{roihemi}.nii.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 64, 64, 36)\n",
      "(64, 64, 36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/milgram/project/turk-browne/users/kp578/CONDA/rtcloud/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "brain=np.load(f\"{cfg.recognition_dir}brain_run10.npy\")\n",
    "print(brain.shape)\n",
    "mask=nib.load(f\"{cfg.recognition_dir}chosenMask.nii.gz\").get_data()\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
