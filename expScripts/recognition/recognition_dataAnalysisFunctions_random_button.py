'''
input: 
    cfg.subjectName
    cfg.dicom_dir # Folder where data is saved 
        # e.g. /gpfs/milgram/project/realtime/DICOM/20201019.rtSynth_pilot001_2.rtSynth_pilot001_2/ 
        # inside which is like 001_000003_000067.dcm

output: save

major steps: 
    figure out the number of runs # should be 8, but need confirmation 
        day1 in cfg.dicom_dir there are 8 runs
        day2 in cfg.dicom_dir there are 2+1+2 runs
        day3 in cfg.dicom_dir there are 2+1+2 runs
        day4 in cfg.dicom_dir there are 2+1+2 runs
        day1 in cfg.dicom_dir there are 8 runs

    # preprocess and alignment
    1. figure out the number of dicoms in each runs, should be the maxTR for each run.
    2. select the middle volume of the first run as the template functional volume and save it inside the cfg and save cfg using pickle
    3. align every other functional volume with templateFunctionalVolume (3dvolreg)
    4. merge the aligned data to the PreprocessedData, finish preprocessing (fslmerge)

    # recog_features.py portion
    5. load the aligned nifti file generated by neurosketch_realtime_preprocess.py
    6. no filter
    7. load mask data and apply mask
    8. load behavior data and push the behavior data back for 2 TRs

    # offlineModelTraining.py portion
    9. load preprocessed and aligned behavior and brain data 
    10. select data with the wanted pattern like AB AC AD BC BD CD 
    11. train correspondng classifier and save the classifier performance and the classifiers themselves.


'''

# import and set up environment
import sys
from subprocess import call
import nibabel as nib
import pydicom as dicom
import numpy as np
import time
import os
from glob import glob
import shutil
import pandas as pd
# from import convertDicomFileToNifti
from rtCommon.imageHandling import convertDicomImgToNifti, readDicomFromFile
from rtCommon.cfg_loading import mkdir,cfg_loading

# setting up code testing environment: 
# from rtCommon.cfg_loading import mkdir,cfg_loading ;cfg = cfg_loading('pilot_sub001.ses1.toml')

def recognition_preprocess(cfg,scan_asTemplate): 
    '''
    purpose: 
        prepare data for the model training code.
    steps:
        convert all dicom files into nii files in the temp dir. 
        find the middle volume of the run1 as the template volume
        align every other functional volume with templateFunctionalVolume (3dvolreg)
    '''
    # select a list of run IDs based on the runRecording.csv, actualRuns would be [1,2] is the 1st and the 3rd runs are recognition runs.
    runRecording = pd.read_csv(f"{cfg.recognition_dir}../runRecording.csv")
    actualRuns = list(runRecording['run'].iloc[list(np.where(1==1*(runRecording['type']=='recognition'))[0])])

    # convert all dicom files into nii files in the temp dir. 
    if os.path.exists(f"{cfg.recognition_dir}run{actualRuns[-1]}.nii") or os.path.exists(f"{cfg.recognition_dir}run{actualRuns[-1]}.nii.gz"):
        pass # 如果检测到已经存在了fslmerge的结果，就不做这一步了 中文
    else:
        tmp_dir=f"{cfg.tmp_folder}{time.time()}/" ; mkdir(tmp_dir)
        dicomFiles=glob(f"{cfg.dicom_dir}/*.dcm") ; dicomFiles.sort()
        for curr_dicom in dicomFiles:
            dicomImg = readDicomFromFile(curr_dicom) # read dicom file
            convertDicomImgToNifti(dicomImg, dicomFilename=f"{tmp_dir}/{curr_dicom.split('/')[-1]}") #convert dicom to nii    
            # os.remove(f"{tmp_dir}/{curr_dicom.split('/')[-1]}") # remove temp dcm file

        # find the middle volume of the run1 as the template volume
        
        scan_asTemplate=str(scan_asTemplate).zfill(6)
        tmp=glob(f"{tmp_dir}001_{scan_asTemplate}*.nii") ; tmp.sort()
        # cfg.templateFunctionalVolume = f"{cfg.recognition_dir}/templateFunctionalVolume.nii" 
        if cfg.session ==1:
            call(f"cp {tmp[int(len(tmp)/2)]} {cfg.templateFunctionalVolume}", shell=True)
            call(f"cp {cfg.templateFunctionalVolume} {cfg.templateFunctionalVolume_converted}", shell=True)
        else:
            # call(f"cp {tmp[int(len(tmp)/2)]} {cfg.templateFunctionalVolume_converted}", shell=True)
            # convert cfg.templateFunctionalVolume to the previous template volume space 
            cmd=f"flirt -ref {cfg.templateFunctionalVolume} \
                -in {tmp[int(len(tmp)/2)]} \
                -out {cfg.templateFunctionalVolume_converted}"
            print(cmd)
            call(cmd,shell=True) 

        # align every other functional volume with templateFunctionalVolume (3dvolreg)
        allTRs=glob(f"{tmp_dir}/001_*.nii") ; allTRs.sort()

        for curr_run in actualRuns:
            outputFileNames=[]
            runTRs=glob(f"{tmp_dir}/001_{str(curr_run).zfill(6)}_*.nii") ; runTRs.sort()
            for curr_TR in runTRs:
                command = f"3dvolreg \
                    -base {cfg.templateFunctionalVolume_converted} \
                    -prefix  {curr_TR[0:-4]}_aligned.nii \
                    {curr_TR}"
                call(command,shell=True)
                outputFileNames.append(f"{curr_TR[0:-4]}_aligned.nii")
            files=''
            for f in outputFileNames:
                files=files+' '+f
            command=f"fslmerge -t {cfg.recognition_dir}run{curr_run}.nii {files}"
            print('running',command)
            call(command, shell=True)

        # remove the tmp folder
        shutil.rmtree(tmp_dir)
            
    '''
    for each run, 
        load behavior data 
        push the behavior data back for 2 TRs
        save the brain TRs with images
        save the behavior data
    '''

    for curr_run_behav,curr_run in enumerate(actualRuns):
        # load behavior data
        behav_data = behaviorDataLoading(cfg,curr_run_behav+1) # behav_data 的数据的TR是从0开始的。brain_data 也是 中文
        #len = 48 ，最后一个TR ID是 142 中文

        # brain data is first aligned by pushed back 2TR(4s)
        brain_data = nib.load(f"{cfg.recognition_dir}run{curr_run}.nii.gz").get_data() ; brain_data=np.transpose(brain_data,(3,0,1,2))
        #len = 144
        Brain_TR=np.arange(brain_data.shape[0]) #假设brain_data 有144个，那么+2之后的Brain_TR就是2，3，。。。，145.一共144个TR。中文
        Brain_TR = Brain_TR + 2

        # select volumes of brain_data by counting which TR is left in behav_data
        Brain_TR=Brain_TR[list(behav_data['TR'])] # original TR begin with 0 #筛选掉无用的TR，由于两个都是从0开始计数的，所以是可以的。 中文
        # 筛选掉之后的Brain_TR长度是 48 最后一个ID是144 中文
        # Brain_TR[-1] 是想要的最后一个TR的ID，看看是否在brain_data里面？如果不在的话，那么删除最后一个Brain_TR，也删除behav里面的最后一行 中文    
        # 如果大脑数据的长度没有行为学数据长（比如大脑只收集到144个TR，然后我现在想要第145个TR的数据，这提醒我千万不要过早结束recognition run） 中文
        if Brain_TR[-1]>=brain_data.shape[0]: # when the brain data is not as long as the behavior data, delete the last row
            print("Warning: brain data is not long enough, don't cut the data collection too soon!!!!")
            Brain_TR = Brain_TR[:-1]
            #behav_data = behav_data.drop([behav_data.iloc[-1].TR])
            behav_data.drop(behav_data.tail(1).index,inplace=True)

        brain_data=brain_data[Brain_TR]
        np.save(f"{cfg.recognition_dir}brain_run{curr_run}.npy", brain_data)
        # save the behavior data
        behav_data.to_csv(f"{cfg.recognition_dir}behav_run{curr_run}.csv")



def recognition_preprocess_2run(cfg,scan_asTemplate): 
    '''
    purpose: 
        prepare the data for 2 recognition runs     (to later(not in this function) get the morphing target function)
        find the functional template image for current session
    steps:
        convert all dicom files into nii files in the temp dir. 
        find the middle volume of the run1 as the template volume, convert this to the previous template volume space and save the converted file as today's functional template (templateFunctionalVolume)
        align every other functional volume with templateFunctionalVolume (3dvolreg)
    '''
    from shutil import copyfile
    from rtCommon.imageHandling import convertDicomFileToNifti
    # convert all dicom files into nii files in the temp dir. 
    tmp_dir=f"{cfg.tmp_folder}{time.time()}/" ; mkdir(tmp_dir)
    dicomFiles=glob(f"{cfg.dicom_dir}/*.dcm") ; dicomFiles.sort()
    # 把cfg.dicom_dir的file复制到tmp folder并且转换成nii
    for curr_dicom in dicomFiles:
        # dicomImg = readDicomFromFile(curr_dicom) # read dicom file
        dicomFilename=f"{tmp_dir}{curr_dicom.split('/')[-1]}"
        copyfile(curr_dicom,dicomFilename)
        niftiFilename = dicomFilename[:-4]+'.nii'
        convertDicomFileToNifti(dicomFilename, niftiFilename)
        # convertDicomImgToNifti(dicomImg, dicomFilename=f"{tmp_dir}{curr_dicom.split('/')[-1]}") #convert dicom to nii    
        # os.remove(f"{tmp_dir}/{curr_dicom.split('/')[-1]}") # remove temp dcm file

    # find the middle volume of the run1 as the template volume
    # here you are assuming that the first run is a good run
    scan_asTemplate=str(scan_asTemplate).zfill(6)
    tmp=glob(f"{tmp_dir}001_{scan_asTemplate}*.nii") ; tmp.sort()
    # print(f"all nii files: {tmp}")
    # call(f"cp {tmp[int(len(tmp)/2)]} {cfg.recognition_dir}t.nii", shell=True)

    # convert cfg.templateFunctionalVolume to the previous template volume space 
    cmd=f"flirt -ref {cfg.templateFunctionalVolume} \
        -in {tmp[int(len(tmp)/2)]} \
        -out {cfg.templateFunctionalVolume_converted}"
    print(cmd)
    call(cmd,shell=True) 
        

    # # align every other functional volume with templateFunctionalVolume (3dvolreg)
    # allTRs=glob(f"{tmp_dir}001_*.nii") ; allTRs.sort()
    # # select a list of run IDs based on the runRecording.csv, actualRuns would be [1,2] is the 1st and the 3rd runs are recognition runs.
    # runRecording = pd.read_csv(f"{cfg.recognition_dir}../runRecording.csv")
    # actualRuns = list(runRecording['run'].iloc[list(np.where(1==1*(runRecording['type']=='recognition'))[0])])[:2]
    # for curr_run in actualRuns:
    #     if not (os.path.exists(f"{cfg.recognition_dir}run{curr_run}.nii.gz") and os.path.exists(f"{cfg.recognition_dir}run{curr_run}.nii")):
    #         outputFileNames=[]
    #         runTRs=glob(f"{tmp_dir}001_{str(curr_run).zfill(6)}_*.nii") ; runTRs.sort()
    #         for curr_TR in runTRs:
    #             command = f"3dvolreg \
    #                 -base {cfg.templateFunctionalVolume_converted} \
    #                 -prefix  {curr_TR[0:-4]}_aligned.nii \
    #                 {curr_TR}"
    #             call(command,shell=True)
    #             outputFileNames.append(f"{curr_TR[0:-4]}_aligned.nii")
    #         files=''
    #         for f in outputFileNames:
    #             files=files+' '+f
    #         command=f"fslmerge -t {cfg.recognition_dir}run{curr_run}.nii {files}"
    #         print('running',command)
    #         call(command, shell=True)


    # remove the tmp folder
    shutil.rmtree(tmp_dir)
            
    # '''
    # for each run, 
    #     load behavior data 
    #     push the behavior data back for 2 TRs
    #     save the brain TRs with images
    #     save the behavior data
    # '''

    # for curr_run_behav,curr_run in enumerate(actualRuns):
    #     # load behavior data
    #     behav_data = behaviorDataLoading(cfg,curr_run_behav+1)

    #     # brain data is first aligned by pushed back 2TR(4s)
    #     brain_data = nib.load(f"{cfg.recognition_dir}run{curr_run}.nii.gz").get_data() ; brain_data=np.transpose(brain_data,(3,0,1,2))
    #     Brain_TR=np.arange(brain_data.shape[0])
    #     Brain_TR = Brain_TR+2

    #     # select volumes of brain_data by counting which TR is left in behav_data
    #     Brain_TR=Brain_TR[list(behav_data['TR'])] # original TR begin with 0
    #     if Brain_TR[-1]>=brain_data.shape[0]: # when the brain data is not as long as the behavior data, delete the last row
    #         Brain_TR = Brain_TR[:-1]
    #         behav_data = behav_data.drop([behav_data.iloc[-1].TR])
    #     brain_data=brain_data[Brain_TR]
    #     np.save(f"{cfg.recognition_dir}brain_run{curr_run}.npy", brain_data)
    #     # save the behavior data
    #     behav_data.to_csv(f"{cfg.recognition_dir}behav_run{curr_run}.csv")



from scipy.stats import zscore
def normalize(X):
    _X=X.copy()
    _X = zscore(_X, axis=0)
    _X[np.isnan(_X)]=0
    return _X

def minimalClass(cfg,testRun=None,recordingTxt=None):
    '''
    purpose: 
        train offline models

    steps:
        load preprocessed and aligned behavior and brain data 
        select data with the wanted pattern like AB AC AD BC BD CD 
        train correspondng classifier and save the classifier performance and the classifiers themselves.
    '''

    import os
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    import sklearn
    import joblib
    import nibabel as nib
    import itertools
    from sklearn.linear_model import LogisticRegression

    def other(target):
        other_objs = [i for i in ['bed', 'bench', 'chair', 'table'] if i not in target]
        return other_objs

    def red_vox(n_vox, prop=0.1):
        return int(np.ceil(n_vox * prop))

    if 'milgram' in os.getcwd():
        main_dir='/gpfs/milgram/project/turk-browne/projects/rtSynth_rt/'
    else:
        main_dir='/Volumes/GoogleDrive/My Drive/Turk_Browne_Lab/rtcloud_kp/'

    working_dir=main_dir
    os.chdir(working_dir)

    '''
    if you read runRecording for current session and found that there are only 4 runs in the current session, 
    you read the runRecording for previous session and fetch the last 4 recognition runs from previous session
    '''
    runRecording = pd.read_csv(f"{cfg.recognition_dir}../runRecording.csv")
    actualRuns = list(runRecording['run'].iloc[list(np.where(1==1*(runRecording['type']=='recognition'))[0])]) # can be [1,2,3,4,5,6,7,8] or [1,2,4,5]
    if len(actualRuns) < 8:
        runRecording_preDay = pd.read_csv(f"{cfg.subjects_dir}{cfg.subjectName}/ses{cfg.session-1}/recognition/../runRecording.csv")
        actualRuns_preDay = list(runRecording_preDay['run'].iloc[list(np.where(1==1*(runRecording_preDay['type']=='recognition'))[0])])[-(8-len(actualRuns)):] # might be [5,6,7,8]
    else: 
        actualRuns_preDay = []

    # assert len(actualRuns_preDay)+len(actualRuns)==8 
    if len(actualRuns_preDay)+len(actualRuns)<8:
        runRecording_prepreDay = pd.read_csv(f"{cfg.subjects_dir}{cfg.subjectName}/ses{cfg.session-2}/recognition/../runRecording.csv")
        actualRuns_prepreDay = list(runRecording_prepreDay['run'].iloc[list(np.where(1==1*(runRecording_prepreDay['type']=='recognition'))[0])])[-(8-len(actualRuns)-len(actualRuns_preDay)):] # might be [5,6,7,8]
    else:
        actualRuns_prepreDay = []

    objects = ['bed', 'bench', 'chair', 'table']

    new_run_indexs=[]
    new_run_index=1 #使用新的run 的index，以便于后面的testRun selection的时候不会重复。正常的话 new_run_index 应该是1，2，3，4，5，6，7，8
    for ii,run in enumerate(actualRuns): # load behavior and brain data for current session
        t = np.load(f"{cfg.recognition_dir}brain_run{run}.npy")
        mask = np.load(f"{cfg.chosenMask}")
        t = t[:,mask==1]
        t = normalize(t)
        brain_data=t if ii==0 else np.concatenate((brain_data,t), axis=0)

        t = pd.read_csv(f"{cfg.recognition_dir}behav_run{run}.csv")
        t['run_num'] = new_run_index
        new_run_indexs.append(new_run_index)
        new_run_index+=1
        behav_data=t if ii==0 else pd.concat([behav_data,t])

    for ii,run in enumerate(actualRuns_preDay): # load behavior and brain data for previous session
        t = np.load(f"{cfg.subjects_dir}{cfg.subjectName}/ses{cfg.session-1}/recognition/brain_run{run}.npy")
        mask = np.load(f"{cfg.chosenMask}")
        t = t[:,mask==1]
        t = normalize(t)
        brain_data = np.concatenate((brain_data,t), axis=0)

        t = pd.read_csv(f"{cfg.subjects_dir}{cfg.subjectName}/ses{cfg.session-1}/recognition/behav_run{run}.csv")
        t['run_num'] = new_run_index
        new_run_indexs.append(new_run_index)
        new_run_index+=1
        behav_data = pd.concat([behav_data,t])

    for ii,run in enumerate(actualRuns_prepreDay): # load behavior and brain data for previous session
        t = np.load(f"{cfg.subjects_dir}{cfg.subjectName}/ses{cfg.session-2}/recognition/brain_run{run}.npy")
        mask = np.load(f"{cfg.chosenMask}")
        t = t[:,mask==1]
        t = normalize(t)
        brain_data = np.concatenate((brain_data,t), axis=0)

        t = pd.read_csv(f"{cfg.subjects_dir}{cfg.subjectName}/ses{cfg.session-2}/recognition/behav_run{run}.csv")
        t['run_num'] = new_run_index
        new_run_indexs.append(new_run_index)
        new_run_index+=1
        behav_data = pd.concat([behav_data,t])

    # FEAT=brain_data.reshape(brain_data.shape[0],-1)

    FEAT=brain_data
    print(f"FEAT.shape={FEAT.shape}")
    assert len(FEAT.shape)==2
    # FEAT_mean=np.mean(FEAT,axis=1)
    # FEAT=(FEAT.T-FEAT_mean).T
    # FEAT_mean=np.mean(FEAT,axis=0)
    # FEAT=FEAT-FEAT_mean
    # FEAT = normalize(FEAT)

    META=behav_data

    # convert item colume to label colume
    imcodeDict={
    'A': 'bed',
    'B': 'chair',
    'C': 'table',
    'D': 'bench'}
    label=[]
    for curr_trial in range(META.shape[0]):
        label.append(imcodeDict[META['Item'].iloc[curr_trial]])
    META['label']=label # merge the label column with the data dataframe

    # Which run to use as test data (leave as None to not have test data)
    # testRun = 0 # when testing: testRun = 2 ; META['run_num'].iloc[:5]=2
    def train4wayClf(META, FEAT):
        runList = np.unique(list(META['run_num']))
        print(f"runList={runList}")
        accList={}
        for testRun in runList:
            trainIX = META['run_num']!=int(testRun)
            testIX = META['run_num']==int(testRun)

            # pull training and test data
            trainX = FEAT[trainIX]
            testX = FEAT[testIX]
            trainY = META.iloc[np.asarray(trainIX)].label
            testY = META.iloc[np.asarray(testIX)].label

            # Train your classifier
            clf = LogisticRegression(penalty='l2',C=1, solver='lbfgs', max_iter=1000, 
                                        multi_class='multinomial').fit(trainX, trainY)
            
            # model_folder = cfg.trainingModel_dir
            # Save it for later use
            # joblib.dump(clf, model_folder +'/{}.joblib'.format(naming))
            
            # Monitor progress by printing accuracy (only useful if you're running a test set)
            acc = clf.score(testX, testY)
            print("acc=", acc)
            accList[testRun] = acc
        print(f"new trained full rotation 4 way accuracy mean={np.mean(list(accList.values()))}")
        if recordingTxt: #if tmp_folder is not None but some string, save the sentence.
            append_file(f"{recordingTxt}",f"new trained full rotation 4 way accuracy mean={np.mean(list(accList.values()))}")
        
        return accList
    accList = train4wayClf(META, FEAT)
    
    # 获得full rotation的2way clf的accuracy 平均值 中文
    accs_rotation=[]
    print(f"new_run_indexs={new_run_indexs}")
    for testRun in new_run_indexs:
        allpairs = itertools.combinations(objects,2)
        accs={}
        # Iterate over all the possible target pairs of objects
        for pair in allpairs:
            # Find the control (remaining) objects for this pair
            altpair = other(pair)
            
            # pull sorted indices for each of the critical objects, in order of importance (low to high)
            # inds = get_inds(FEAT, META, pair, testRun=testRun)
            
            # Find the number of voxels that will be left given your inclusion parameter above
            # nvox = red_vox(FEAT.shape[1], include)
            
            for obj in pair:
                # foil = [i for i in pair if i != obj][0]
                for altobj in altpair:
                    # establish a naming convention where it is $TARGET_$CLASSIFICATION
                    # Target is the NF pair (e.g. bed/bench)
                    # Classificationis is btw one of the targets, and a control (e.g. bed/chair, or bed/table, NOT bed/bench)
                    naming = '{}{}_{}{}'.format(pair[0], pair[1], obj, altobj)

                    if testRun:
                        trainIX = ((META['label']==obj) | (META['label']==altobj)) & (META['run_num']!=int(testRun))
                        testIX = ((META['label']==obj) | (META['label']==altobj)) & (META['run_num']==int(testRun))
                    else:
                        trainIX = ((META['label']==obj) | (META['label']==altobj))
                        testIX = ((META['label']==obj) | (META['label']==altobj))

                    # pull training and test data
                    trainX = FEAT[trainIX]
                    testX = FEAT[testIX]
                    trainY = META.iloc[np.asarray(trainIX)].label
                    testY = META.iloc[np.asarray(testIX)].label

                    assert len(np.unique(trainY))==2

                    # Train your classifier
                    clf = LogisticRegression(penalty='l2',C=1, solver='lbfgs', max_iter=1000, 
                                                multi_class='multinomial').fit(trainX, trainY)
                    
                    model_folder = cfg.trainingModel_dir
                    # Save it for later use
                    # joblib.dump(clf, model_folder +'/{}.joblib'.format(naming))
                    
                    # Monitor progress by printing accuracy (only useful if you're running a test set)
                    acc = clf.score(testX, testY)
                    print(naming, acc)
                    accs[naming]=acc
        print(f"testRun = {testRun} : average 2 way clf accuracy={np.mean(list(accs.values()))}")
        accs_rotation.append(np.mean(list(accs.values())))
    print(f"mean of 2 way clf acc full rotation = {np.mean(accs_rotation)}")
    if recordingTxt: #if tmp_folder is not None but some string, save the sentence.
        append_file(f"{recordingTxt}",f"mean of 2 way clf acc full rotation = {np.mean(accs_rotation)}")


    # 用所有数据训练要保存并且使用的模型：
    allpairs = itertools.combinations(objects,2)
    accs={}
    # Iterate over all the possible target pairs of objects
    for pair in allpairs:
        # Find the control (remaining) objects for this pair
        altpair = other(pair)
        for obj in pair:
            # foil = [i for i in pair if i != obj][0]
            for altobj in altpair:
                # establish a naming convention where it is $TARGET_$CLASSIFICATION
                # Target is the NF pair (e.g. bed/bench)
                # Classificationis is btw one of the targets, and a control (e.g. bed/chair, or bed/table, NOT bed/bench)
                naming = '{}{}_{}{}'.format(pair[0], pair[1], obj, altobj)

                trainIX = ((META['label']==obj) | (META['label']==altobj))
                testIX = ((META['label']==obj) | (META['label']==altobj))

                # pull training and test data
                trainX = FEAT[trainIX]
                testX = FEAT[testIX]
                trainY = META.iloc[np.asarray(trainIX)].label
                testY = META.iloc[np.asarray(testIX)].label

                assert len(np.unique(trainY))==2

                # Train your classifier
                clf = LogisticRegression(penalty='l2',C=1, solver='lbfgs', max_iter=1000, 
                                            multi_class='multinomial').fit(trainX, trainY)
                
                model_folder = cfg.trainingModel_dir
                # Save it for later use
                joblib.dump(clf, model_folder +'/{}.joblib'.format(naming))
                
                # Monitor progress by printing accuracy (only useful if you're running a test set)
                acc = clf.score(testX, testY)
                print(naming, acc)
                accs[naming]=acc
    print(f"average 2 way clf accuracy={np.mean(list(accs.values()))}")

    return accs

def append_file(fileName,text):
    # Open a file with access mode 'a'
    file_object = open(fileName, 'a')
    # Append 'hello' at the end of file
    file_object.write(f" \n\n {text}")
    # Close the file
    file_object.close()

def compareScore(cfg,testRun=None): 
    # 这个函数是从minimalClass修改过来的。目的是为了使用ses4的模型来对比ses5的前两个recognition run和后两个recognition run 的testing score
    # cfg 使用的是ses5的cfg，可以使用cfg.usingModel_dir 的模型，但是使用的是ses5的数据
    # http://localhost:8125/notebooks/projects/rtSynth_rt/archive/compareScore.ipynb
    import os
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    import sklearn
    import joblib
    import nibabel as nib
    import itertools
    from sklearn.linear_model import LogisticRegression

    def other(target):
        other_objs = [i for i in ['bed', 'bench', 'chair', 'table'] if i not in target]
        return other_objs
    def red_vox(n_vox, prop=0.1):
        return int(np.ceil(n_vox * prop))
    if 'milgram' in os.getcwd():
        main_dir='/gpfs/milgram/project/turk-browne/projects/rtSynth_rt/'
    else:
        main_dir='/Volumes/GoogleDrive/My Drive/Turk_Browne_Lab/rtcloud_kp/'
    working_dir=main_dir
    os.chdir(working_dir)

    '''
    if you read runRecording for current session and found that there are only 4 runs in the current session, 
    you read the runRecording for previous session and fetch the last 4 recognition runs from previous session
    '''
    runRecording = pd.read_csv(f"{cfg.recognition_dir}../runRecording.csv")
    actualRuns = list(runRecording['run'].iloc[list(np.where(1==1*(runRecording['type']=='recognition'))[0])]) # can be [1,2,3,4,5,6,7,8] or [1,2,4,5]
    
    objects = ['bed', 'bench', 'chair', 'table']

    new_run_indexs=[]
    new_run_index=1 #使用新的run 的index，以便于后面的testRun selection的时候不会重复。正常的话 new_run_index 应该是1，2，3，4，5，6，7，8
    for ii,run in enumerate(actualRuns): # load behavior and brain data for current session
        t = np.load(f"{cfg.recognition_dir}brain_run{run}.npy")
        mask = np.load(f"{cfg.chosenMask}")
        t = t[:,mask==1]
        t = normalize(t)
        brain_data=t if ii==0 else np.concatenate((brain_data,t), axis=0)

        t = pd.read_csv(f"{cfg.recognition_dir}behav_run{run}.csv")
        t['run_num'] = new_run_index
        new_run_indexs.append(new_run_index)
        new_run_index+=1
        behav_data=t if ii==0 else pd.concat([behav_data,t])
    FEAT=brain_data
    print(f"FEAT.shape={FEAT.shape}")
    assert len(FEAT.shape)==2
    META=behav_data

    # convert item colume to label colume
    imcodeDict={
    'A': 'bed',
    'B': 'chair',
    'C': 'table',
    'D': 'bench'}
    label=[]
    for curr_trial in range(META.shape[0]):
        label.append(imcodeDict[META['Item'].iloc[curr_trial]])
    META['label']=label # merge the label column with the data dataframe

    # Which run to use as test data (leave as None to not have test data)
    # testRun = 0 # when testing: testRun = 2 ; META['run_num'].iloc[:5]=2
    def train4wayClf(META, FEAT):
        runList = np.unique(list(META['run_num']))
        print(f"runList={runList}")
        accList={}
        for testRun in runList:
            trainIX = META['run_num']!=int(testRun)
            testIX = META['run_num']==int(testRun)

            # pull training and test data
            trainX = FEAT[trainIX]
            testX = FEAT[testIX]
            trainY = META.iloc[np.asarray(trainIX)].label
            testY = META.iloc[np.asarray(testIX)].label

            # Train your classifier
            clf = LogisticRegression(penalty='l2',C=1, solver='lbfgs', max_iter=1000, 
                                        multi_class='multinomial').fit(trainX, trainY)
            
            # model_folder = cfg.trainingModel_dir
            # Save it for later use
            # joblib.dump(clf, model_folder +'/{}.joblib'.format(naming))
            
            # Monitor progress by printing accuracy (only useful if you're running a test set)
            acc = clf.score(testX, testY)
            print("acc=", acc)
            accList[testRun] = acc
        print(f"new trained full rotation 4 way accuracy mean={np.mean(list(accList.values()))}")
        
        return accList
    # accList = train4wayClf(META, FEAT)
    
    # 获得full rotation的2way clf的accuracy 平均值 中文
    accs_rotation=[]
    print(f"new_run_indexs={new_run_indexs}")
    # for testRun in new_run_indexs:
    #     allpairs = itertools.combinations(objects,2)
    #     accs={}
    #     # Iterate over all the possible target pairs of objects
    #     for pair in allpairs:
    #         # Find the control (remaining) objects for this pair
    #         altpair = other(pair)
            
    #         # pull sorted indices for each of the critical objects, in order of importance (low to high)
    #         # inds = get_inds(FEAT, META, pair, testRun=testRun)
            
    #         # Find the number of voxels that will be left given your inclusion parameter above
    #         # nvox = red_vox(FEAT.shape[1], include)
            
    #         for obj in pair:
    #             # foil = [i for i in pair if i != obj][0]
    #             for altobj in altpair:
    #                 # establish a naming convention where it is $TARGET_$CLASSIFICATION
    #                 # Target is the NF pair (e.g. bed/bench)
    #                 # Classificationis is btw one of the targets, and a control (e.g. bed/chair, or bed/table, NOT bed/bench)
    #                 naming = '{}{}_{}{}'.format(pair[0], pair[1], obj, altobj)

    #                 if testRun:
    #                     trainIX = ((META['label']==obj) | (META['label']==altobj)) & (META['run_num']!=int(testRun))
    #                     testIX = ((META['label']==obj) | (META['label']==altobj)) & (META['run_num']==int(testRun))
    #                 else:
    #                     trainIX = ((META['label']==obj) | (META['label']==altobj))
    #                     testIX = ((META['label']==obj) | (META['label']==altobj))

    #                 # pull training and test data
    #                 trainX = FEAT[trainIX]
    #                 testX = FEAT[testIX]
    #                 trainY = META.iloc[np.asarray(trainIX)].label
    #                 testY = META.iloc[np.asarray(testIX)].label

    #                 assert len(np.unique(trainY))==2

    #                 # Train your classifier
    #                 clf = LogisticRegression(penalty='l2',C=1, solver='lbfgs', max_iter=1000, 
    #                                             multi_class='multinomial').fit(trainX, trainY)
                    
    #                 model_folder = cfg.trainingModel_dir
    #                 # Save it for later use
    #                 # joblib.dump(clf, model_folder +'/{}.joblib'.format(naming))
                    
    #                 # Monitor progress by printing accuracy (only useful if you're running a test set)
    #                 acc = clf.score(testX, testY)
    #                 print(naming, acc)
    #                 accs[naming]=acc
    #     print(f"testRun = {testRun} : average 2 way clf accuracy={np.mean(list(accs.values()))}")
    #     accs_rotation.append(np.mean(list(accs.values())))
    # print(f"mean of 2 way clf acc full rotation = {np.mean(accs_rotation)}")

    # 用所有数据训练要保存并且使用的模型：
    allpairs = itertools.combinations(objects,2)
    accs={}
    # Iterate over all the possible target pairs of objects
    for pair in allpairs:
        # Find the control (remaining) objects for this pair
        altpair = other(pair)
        for obj in pair:
            # foil = [i for i in pair if i != obj][0]
            for altobj in altpair:
                # establish a naming convention where it is $TARGET_$CLASSIFICATION
                # Target is the NF pair (e.g. bed/bench)
                # Classificationis is btw one of the targets, and a control (e.g. bed/chair, or bed/table, NOT bed/bench)
                naming = '{}{}_{}{}'.format(pair[0], pair[1], obj, altobj)

                trainIX = ((META['label']==obj) | (META['label']==altobj))
                testIX = ((META['label']==obj) | (META['label']==altobj))

                # pull training and test data
                trainX = FEAT[trainIX]
                testX = FEAT[testIX]
                trainY = META.iloc[np.asarray(trainIX)].label
                testY = META.iloc[np.asarray(testIX)].label

                assert len(np.unique(trainY))==2

                # Train your classifier
                # clf = LogisticRegression(penalty='l2',C=1, solver='lbfgs', max_iter=1000, 
                                            # multi_class='multinomial').fit(trainX, trainY)
                
                # Save it for later use
                clf = joblib.load(clf, cfg.usingModel_dir +'/{}.joblib'.format(naming))
                
                # Monitor progress by printing accuracy (only useful if you're running a test set)
                acc = clf.score(testX, testY)
                print(naming, acc)
                accs[naming]=acc
    print(f"average 2 way clf accuracy={np.mean(list(accs.values()))}")

    return accs
                                

def behaviorDataLoading(cfg,curr_run):
    '''
    extract the labels which is selected by the subject and coresponding TR and time
    check if the subject's response is correct. When Item is A,bed, response should be 1, or it is wrong
    '''
    behav_data = pd.read_csv(f"{cfg.recognition_dir}{cfg.subjectName}_{curr_run}.csv")

    # the item(imcode) colume of the data represent each image in the following correspondence
    imcodeDict={
    'A': 'bed',
    'B': 'chair',
    'C': 'table',
    'D': 'bench'}

    # When the imcode code is "A", the correct response should be '1', "B" should be '2'
    correctResponseDict={
    'A': 1,
    'B': 2,
    'C': 1,
    'D': 2}

    SwitchCorrectResponseDict={
    'A': 2,
    'B': 1,
    'C': 2,
    'D': 1}

    # 由于每次保存按钮的时候所保存的这个flag（switchButtonOrientation）对应的是下一个trial的switchButtonOrientation，
    # 因此采用把switchButtonOrientation往前推一个的方法纠正。  中文
    switchButtonOrientation = [None] + list(behav_data['switchButtonOrientation'])
    switchButtonOrientation = switchButtonOrientation[:-1]
    behav_data['switchButtonOrientation'] = switchButtonOrientation
    # extract the labels which is selected by the subject and coresponding TR and time
    behav_data = behav_data[['TR', 'image_on', 'Resp',  'Item', 'switchButtonOrientation']] # the TR, the real time it was presented, 

    # 为了处理 情况 A.被试的反应慢了一个TR，或者 B.两个按钮都被按了(这种情况下按照第二个按钮处理)
    # 现在的问题是”下一个TR“可能超过了behav_data的长度
    # this for loop is to deal with the situation where Resp is late for 1 TR, or two buttons are pressed. 
    # when Resp is late for 1 TR, set the current Resp as the later Response.
    # when two buttons are pressed, set the current Resp as the later Response because the later one should be the real choice
    for curr_trial in range(behav_data.shape[0]):
        if behav_data['Item'].iloc[curr_trial]  in ["A","B","C","D"]:
            if curr_trial+1<behav_data.shape[0]: # 为了防止”下一个TR“超过behav_data的长度  中文
                if behav_data['Resp'].iloc[curr_trial+1] in [1.0,2.0]:
                    behav_data['Resp'].iloc[curr_trial]=behav_data['Resp'].iloc[curr_trial+1]


    behav_data=behav_data.dropna(subset=['Item'])

    # check if the subject's response is correct. When Item is A,bed, response should be 1, or it is wrong
    isCorrect=[]
    for curr_trial in range(behav_data.shape[0]):
        if behav_data['switchButtonOrientation'].iloc[curr_trial]:
            isCorrect.append(SwitchCorrectResponseDict[behav_data['Item'].iloc[curr_trial]]==behav_data['Resp'].iloc[curr_trial])
        else:
            isCorrect.append(correctResponseDict[behav_data['Item'].iloc[curr_trial]]==behav_data['Resp'].iloc[curr_trial])

    print(f"behavior pressing accuracy for run {curr_run} = {np.mean(isCorrect)}")

    behav_data['isCorrect']=isCorrect # merge the isCorrect clumne with the data dataframe
    behav_data['subj']=[cfg.subjectName for i in range(len(behav_data))]
    behav_data['run_num']=[int(curr_run) for i in range(len(behav_data))]
    behav_data=behav_data[behav_data['isCorrect']] # discard the trials where the subject made wrong selection
    return behav_data



def classifierProb(clf,X,Y):
    ID=np.where((clf.classes_==Y)*1==1)[0][0]
    p = clf.predict_proba(X)[:,ID]
    return p



def Wait(waitfor, delay=1):
    while not os.path.exists(waitfor):
        time.sleep(delay)
        print('waiting for {}'.format(waitfor))
        

def fetchXnat(sess_ID):
    "rtSynth_sub001"
    "rtSynth_sub001_ses2"
    import subprocess
    from subprocess import call
    rawPath="/gpfs/milgram/project/turk-browne/projects/rtSynth_rt/expScripts/recognition/recognitionDataAnalysis/raw/"
    proc = subprocess.Popen([f'sbatch {rawPath}../fetchXNAT.sh {sess_ID}'],shell=True)

    Wait(f"{rawPath}{sess_ID}.zip")
    call(f"unzip {rawPath}{sess_ID}.zip")
    time.sleep(10)
    proc = subprocess.Popen([f'sbatch {rawPath}../change2nifti.sh {sess_ID}'],shell=True)

    # furthur work need to be done with this resulting nifti folder

def greedyMask(cfg,N=78): # N used to be 31, 25
    '''
    purpose:
        starting from N ROIs, get the best performed ROI combination in a greedy way
        this code is aggregate_greedy.py adapted to match rtcloud
    steps:
        load the N ROIs from result of neurosketch dataset
        train the model using the NROIs and get the accuracy.

        get the N combinations of N-1 ROIs
        retrain the model and get the accuracy for these N combinations

        get the N-1 combinations of N-2 ROIs
        retrain the model and get the accuracy for these N-1 combinations

        when everything is finished, find the best ROI and save as cfg.chosenMask
        
    '''
    import os
    print(f"conda env={os.environ['CONDA_DEFAULT_ENV']}") 
    import numpy as np
    import nibabel as nib
    import sys
    sys.path.append('/gpfs/milgram/project/turk-browne/projects/rtSynth_rt/')
    import time
    import pandas as pd
    from sklearn.linear_model import LogisticRegression
    import itertools
    # from tqdm import tqdm
    import pickle5 as pickle
    import subprocess
    from subprocess import call
    def save_obj(obj, name):
        with open(name + '.pkl', 'wb') as f:
            pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)

    def load_obj(name):
        with open(name + '.pkl', 'rb') as f:
            return pickle.load(f)

    # What subject are you running
    '''
    Takes args (in order):
        subject (e.g. sub001)
        dataSource (e.g. realtime)
        roiloc (wang2014 or schaefer2018)
        N (the number of parcels or ROIs to start with)
    '''


    from rtCommon.cfg_loading import mkdir,cfg_loading
    # config="sub001.ses1.toml"
    # cfg = cfg_loading(config)

    subject,dataSource,roiloc,N=cfg.subjectName,"realtime","schaefer2018",N
    # subject,dataSource,roiloc,N=sys.argv[1],sys.argv[2],sys.argv[3],int(sys.argv[4])

    print("Running subject {}, with {} as a data source, {}, starting with {} ROIs".format(subject, dataSource, roiloc, N))

    funcdata = cfg.recognition_dir + "brain_run{run}.npy"
    metadata = cfg.recognition_dir + "behav_run{run}.csv"

    topN = load_obj(f"{cfg.recognition_expScripts_dir}top{N}ROIs")
    print(f"len(topN)={len(topN)}")
    print(f"GMschaefer_ topN loaded from neurosketch={topN}")

    def Wait(waitfor, delay=1):
        while not os.path.exists(waitfor):
            time.sleep(delay)
            print('waiting for {}'.format(waitfor))

    imcodeDict={"A": "bed", "B": "Chair", "C": "table", "D": "bench"}

    def getMask(topN, cfg):
        for pn, parc in enumerate(topN):
            _mask = nib.load(f"{cfg.subjects_dir}{cfg.subjectName}/ses1/recognition/mask/GMschaefer_{parc}")
            # schaefer_56.nii.gz
            aff = _mask.affine
            _mask = _mask.get_data()
            _mask = _mask.astype(int)
            # say some things about the mask.
            mask = _mask if pn == 0 else mask + _mask
            mask[mask>0] = 1
        return mask

    mask=getMask(topN, cfg)

    print('mask dimensions: {}'. format(mask.shape))
    print('number of voxels in mask: {}'.format(np.sum(mask)))


    runRecording = pd.read_csv(f"{cfg.recognition_dir}../runRecording.csv")
    actualRuns = list(runRecording['run'].iloc[list(np.where(1==1*(runRecording['type']=='recognition'))[0])]) # can be [1,2,3,4,5,6,7,8] or [1,2,4,5]
    if len(actualRuns) < 8:
        runRecording_preDay = pd.read_csv(f"{cfg.subjects_dir}{cfg.subjectName}/ses{cfg.session-1}/recognition/../runRecording.csv")
        actualRuns_preDay = list(runRecording_preDay['run'].iloc[list(np.where(1==1*(runRecording_preDay['type']=='recognition'))[0])])[-(8-len(actualRuns)):] # might be [5,6,7,8]
    else: 
        actualRuns_preDay = []

    # assert len(actualRuns_preDay)+len(actualRuns)==8 
    if len(actualRuns_preDay)+len(actualRuns)<8:
        runRecording_prepreDay = pd.read_csv(f"{cfg.subjects_dir}{cfg.subjectName}/ses{cfg.session-2}/recognition/../runRecording.csv")
        actualRuns_prepreDay = list(runRecording_prepreDay['run'].iloc[list(np.where(1==1*(runRecording_prepreDay['type']=='recognition'))[0])])[-(8-len(actualRuns)-len(actualRuns_preDay)):] # might be [5,6,7,8]
    else:
        actualRuns_prepreDay = []

    objects = ['bed', 'bench', 'chair', 'table']

    brain_data=[]
    behav_data=[]
    for ii,run in enumerate(actualRuns): # load behavior and brain data for current session
        t = np.load(f"{cfg.recognition_dir}brain_run{run}.npy")
        t = normalize(t)
        brain_data.append(t)

        t = pd.read_csv(f"{cfg.recognition_dir}behav_run{run}.csv")
        t=list(t['Item'])
        behav_data.append(t)
    for ii,run in enumerate(actualRuns_preDay): # load behavior and brain data for previous session
        t = np.load(f"{cfg.subjects_dir}{cfg.subjectName}/ses{cfg.session-1}/recognition/brain_run{run}.npy")
        t = normalize(t)
        brain_data.append(t)

        t = pd.read_csv(f"{cfg.subjects_dir}{cfg.subjectName}/ses{cfg.session-1}/recognition/behav_run{run}.csv")
        t=list(t['Item'])
        behav_data.append(t)
    for ii,run in enumerate(actualRuns_prepreDay): # load behavior and brain data for previous session
        t = np.load(f"{cfg.subjects_dir}{cfg.subjectName}/ses{cfg.session-2}/recognition/brain_run{run}.npy")
        t = normalize(t)
        brain_data.append(t)

        t = pd.read_csv(f"{cfg.subjects_dir}{cfg.subjectName}/ses{cfg.session-2}/recognition/behav_run{run}.csv")
        t=list(t['Item'])
        behav_data.append(t)


    tmp_folder = f"tmp__folder_{time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime(time.time()))}" #tmp__folder
    mkdir(f"{cfg.projectDir}{tmp_folder}")
    save_obj([brain_data,behav_data],f"{cfg.projectDir}{tmp_folder}/{subject}_{dataSource}_{roiloc}_{N}") #{len(topN)}_{i}

    def wait(tmpFile):
        while not os.path.exists(tmpFile+'_result.npy'):
            time.sleep(5)
            print(f"waiting for {tmpFile}_result.npy\n")
        return np.load(tmpFile+'_result.npy')

    def numOfRunningJobs():
        # subprocess.Popen(['squeue -u kp578 | wc -l > squeue.txt'],shell=True) # sl_result = Class(_runs, bcvar)
        randomID=str(time.time())
        # print(f"squeue -u kp578 | wc -l > squeue/{randomID}.txt")
        call(f'squeue -u kp578 | wc -l > {cfg.projectDir}squeue/{randomID}.txt',shell=True)
        numberOfJobsRunning = int(open(f"{cfg.projectDir}squeue/{randomID}.txt", "r").read())
        print(f"numberOfJobsRunning={numberOfJobsRunning}")
        return numberOfJobsRunning

    def Class(brain_data,behav_data):
        # metas = bcvar[0]
        # data4d = data[0]
        print([t.shape for t in brain_data])

        accs = []
        for run in range(8):
            testX = brain_data[run]
            testY = behav_data[run]

            trainX=np.zeros((1,1))
            for i in range(8):
                if i !=run:
                    trainX=brain_data[i] if trainX.shape==(1,1) else np.concatenate((trainX,brain_data[i]),axis=0)

            trainY = []
            for i in range(8):
                if i != run:
                    trainY.extend(behav_data[i])
            clf = LogisticRegression(penalty='l2',C=1, solver='lbfgs', max_iter=1000, 
                                    multi_class='multinomial').fit(trainX, trainY)
                    
            # Monitor progress by printing accuracy (only useful if you're running a test set)
            acc = clf.score(testX, testY)
            accs.append(acc)
        
        return np.mean(accs)

    if not os.path.exists(f"{cfg.projectDir}{tmp_folder}/{subject}_{N}_{roiloc}_{dataSource}_{len(topN)}.pkl"):
        brain_data = [t[:,mask==1] for t in brain_data]
        # _runs = [runs[:,mask==1]]
        print("Runs shape", [t.shape for t in brain_data])
        slstart = time.time()
        sl_result = Class(brain_data, behav_data)
        print(f"passed {time.time()-slstart}s for training")
        save_obj({"subject":subject,
        "startFromN":N,
        "currNumberOfROI":len(topN),
        "bestAcc":sl_result, # this is the sl_result for the topN, not the bestAcc, bestAcc is for the purpose of keeping consistent with others
        "bestROIs":topN},# this is the topN, not the bestROIs, bestROIs is for the purpose of keeping consistent with others
        f"{cfg.projectDir}{tmp_folder}/{subject}_{N}_{roiloc}_{dataSource}_{len(topN)}"
        )

    if os.path.exists(f"{cfg.projectDir}{tmp_folder}/{subject}_{N}_{roiloc}_{dataSource}_{1}.pkl"):
        print(f"{cfg.projectDir}{tmp_folder}/{subject}_{N}_{roiloc}_{dataSource}_1.pkl exists")
        raise Exception('runned or running')

    # N-1
    def next(topN):
        print(f"len(topN)={len(topN)}")
        print(f"topN={topN}")

        if len(topN)==1:
            return None
        else:
            allpairs = itertools.combinations(topN,len(topN)-1)
            topNs=[]
            sl_results=[]
            tmpFiles=[]
            while os.path.exists(f"{cfg.projectDir}{tmp_folder}/holdon.npy"):
                time.sleep(10)
                print(f"sleep for 10s ; waiting for ./{tmp_folder}/holdon.npy to be deleted")
            np.save(f"{cfg.projectDir}{tmp_folder}/holdon",1)

            # 对于每一个round，提交一个job array，然后等待这个job array完成之后再进行下一轮
            # 具体的方法是首先保存需要的input，也就是这一轮需要用到的tmpFile，然后再将tmpFile除了之外的字符串输入
            skip_flag=0
            for i,_topN in enumerate(allpairs):
                tmpFile=f"{cfg.projectDir}{tmp_folder}/{subject}_{N}_{roiloc}_{dataSource}_{len(topN)}_{i}"
                print(f"tmpFile={tmpFile}")
                topNs.append(_topN)
                tmpFiles.append(tmpFile)

                if not os.path.exists(tmpFile+'_result.npy'):
                    # prepare brain data(runs) mask and behavior data(bcvar) 

                    save_obj([_topN,subject,dataSource,roiloc,N], tmpFile)

                    # print("kp2")
                    # numberOfJobsRunning = numOfRunningJobs()
                    # print("kp3")
                    # while numberOfJobsRunning > 400: # 300 is not filling it up
                    #     print("kp4 300")
                    #     print("waiting 10, too many jobs running") ; time.sleep(10)
                    #     numberOfJobsRunning = numOfRunningJobs()
                    #     print("kp5")

                    # get the evidence for the current mask
                    # cmd=f'sbatch --requeue {cfg.recognition_expScripts_dir}class.sh {tmpFile}'
                    # print(cmd)
                    # proc = subprocess.Popen([cmd],shell=True) # sl_result = Class(_runs, bcvar) 
                    # print("kp6")
                else:
                    print(tmpFile+'_result.npy exists!')
                    skip_flag+=1

            if skip_flag!=(i+1): # 如果有一个不存在，就需要跑一跑
                command=f'sbatch --array=1-{i+1} {cfg.recognition_expScripts_dir}class.sh ./{tmp_folder}/{subject}_{N}_{roiloc}_{dataSource}_{len(topN)}_'
                print(command)
                proc = subprocess.Popen([command], shell=True) # sl_result = Class(_runs, bcvar) 
            else:
                command=f'sbatch --array=1-{i+1} {cfg.recognition_expScripts_dir}class.sh ./{tmp_folder}/{subject}_{N}_{roiloc}_{dataSource}_{len(topN)}_'
                print(f"skip {command}")

            os.remove(f"{cfg.projectDir}{tmp_folder}/holdon.npy")

            # wait for everything to be finished and make a summary to find the best performed megaROI
            sl_results=[]
            for tmpFile in tmpFiles:
                sl_result=wait(tmpFile)
                sl_results.append(sl_result)
            print(f"sl_results={sl_results}")
            print(f"max(sl_results)=={max(sl_results)}")
            maxID=np.where(sl_results==max(sl_results))[0][0]
            save_obj({"subject":subject,
            "startFromN":N,
            "currNumberOfROI":len(topN)-1,
            "bestAcc":max(sl_results),
            "bestROIs":topNs[maxID]},
            f"{cfg.projectDir}{tmp_folder}/{subject}_{N}_{roiloc}_{dataSource}_{len(topN)-1}"
            )
            print(f"bestAcc={max(sl_results)} For {len(topN)-1} = {cfg.projectDir}{tmp_folder}/{subject}_{N}_{roiloc}_{dataSource}_{len(topN)-1}")
            tmpFiles=next(topNs[maxID])
            return 0
    tmpFiles=next(topN)



    # when every mask has run, find the best mask and save as the chosenMask
    roiloc="schaefer2018"
    dataSource="realtime"
    subjects=[cfg.subjectName]
    N=N
    GreedyBestAcc=np.zeros((len(subjects),N+1))
    GreedyBestAcc[GreedyBestAcc==0]=None
    for ii,subject in enumerate(subjects):
        for len_topN_1 in range(N-1,0,-1):
            try:
                # print(f"./{tmp_folder}/{subject}_{N}_{roiloc}_{dataSource}_{len_topN_1}")
                di = load_obj(f"{cfg.projectDir}{tmp_folder}/{subject}_{N}_{roiloc}_{dataSource}_{len_topN_1}")
                GreedyBestAcc[ii,len_topN_1-1] = di['bestAcc']
            except:
                pass
    GreedyBestAcc=GreedyBestAcc.T

    # import matplotlib.pyplot as plt
    # plt.imshow(GreedyBestAcc)
    # _=plt.figure()
    # for i in range(GreedyBestAcc.shape[0]):
    #     plt.scatter([i]*GreedyBestAcc.shape[1],GreedyBestAcc[i],c='g',s=2)
    # plt.plot(np.arange(GreedyBestAcc.shape[0]),np.nanmean(GreedyBestAcc,axis=1))

    performance_mean = np.nanmean(GreedyBestAcc,axis=1)
    bestID=np.where(performance_mean==max(performance_mean))[0][0]
    di = load_obj(f"./{tmp_folder}/{subject}_{N}_{roiloc}_{dataSource}_{bestID+1}")
    print(f"bestID={bestID}; best Acc = {di['bestAcc']}")
    print(f"bestROIs={di['bestROIs']}")
    recordingTxt=f"{tmp_folder}/recording.txt"
    append_file(recordingTxt,f"bestID={bestID}; best Acc = {di['bestAcc']}")
    append_file(recordingTxt,f"bestROIs={di['bestROIs']}")

    mask = getMask(di['bestROIs'],cfg)
    np.save(cfg.chosenMask,mask)
    return recordingTxt

def AdaptiveThreshold(cfg,ThresholdLog):
    # if Catalin_current_session_design:
    #     ThresholdLog_curr_ses=ThresholdLog[ThresholdLog['session']==cfg.session]
    #     SuccessList = list(ThresholdLog_curr_ses["successful_trials"]) #成功列表    

    ThresholdList = list(ThresholdLog['threshold'])
    SuccessList = list(ThresholdLog["successful_trials"]) #成功列表

    # 如果现在是第1个session的第一个feedback training run
    # threshold=0.6
    if cfg.session == 2 and cfg.run == 1:
        threshold=0.6

    # 如果现在是第N个session的第一个feedback training run
    # threshold=前一天的最后一个threshold
    elif cfg.run == 1:
        try:
            threshold=ThresholdList[-1]
        except:
            threshold=0.6 #在极端情况下，我可能第二个session没有能够运行feedback session，就必须在第三个session的时候的第一个run才产生第一个threshold
    else:
        change = 0
        threshold=ThresholdList[-1]

        # 如果之前的1个run的进步是<=1
        # threshold=threshold-5%
        if SuccessList[-1] <= 1:
            change = change - 0.05

        # 如果之前的1个run的进步全部>=11
        # threshold=threshold+5%
        if SuccessList[-1] >= 11:
            change = change + 0.05

        if len(SuccessList)>=3:
            # 如果之前的3个run的进步全部<=3
            # threshold=threshold-5%
            if SuccessList[-1] <= 3 and SuccessList[-2] <= 3 and SuccessList[-3] <= 3:
                change = change - 0.05

            # 如果之前的3个run的进步全部>=9
            # threshold=threshold+5%
            elif SuccessList[-1] >= 9 and SuccessList[-2] >= 9 and SuccessList[-3] >= 9:
                change = change + 0.05

        if len(SuccessList)>=5:
            # 如果之前的5个run的进步全部<=5
            # threshold=threshold-5%
            if SuccessList[-1] <= 5 and SuccessList[-2] <= 5 and SuccessList[-3] <= 5 and SuccessList[-4] <= 5 and SuccessList[-5] <= 5:
                change = change - 0.05

            # 如果之前的5个run的进步全部>=7
            # threshold=threshold+5%
            elif SuccessList[-1] >= 7 and SuccessList[-2] >= 7 and SuccessList[-3] >= 7 and SuccessList[-4] >= 7 and SuccessList[-5] >= 7:
                change = change + 0.05

        # 如果之前的任意个run的进步全部【6】
        # threshold=threshold
        if SuccessList[-1] == 6:
            change = 0

        if change > 0.05:
            change = 0.05
        if change < -0.05:
            change = -0.05
        threshold = threshold + change

    # 不要越界
    if threshold>0.9:
        threshold=0.9
    if threshold<0.4:
        threshold=0.4

    # 如果这个run已经跑过了，给出这个error提醒。
    print(f"len(ThresholdLog[(ThresholdLog['session']==cfg.session) & (ThresholdLog['run']==cfg.run)])={len(ThresholdLog[(ThresholdLog['session']==cfg.session) & (ThresholdLog['run']==cfg.run)])}")
    if len(ThresholdLog[(ThresholdLog['session']==cfg.session) & (ThresholdLog['run']==cfg.run)])>0: #more robust than     # if ThresholdLog['session'].iloc[-1]==cfg.session and ThresholdLog['run'].iloc[-1]==cfg.run:
        print(f"this run exists! edit {cfg.adaptiveThreshold}")
        raise Exception(f"this run exists! edit {cfg.adaptiveThreshold}") 

    ThresholdLog = ThresholdLog.append({
        'sub':cfg.subjectName, 
        'session':cfg.session, 
        'run':cfg.run, 
        'threshold':threshold},
        ignore_index=True)

    return ThresholdLog
